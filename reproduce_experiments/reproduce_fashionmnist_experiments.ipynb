{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FashionMNIST Classification Experiments & Ablations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_CPP_MIN_LOG_LEVEL=3\n",
      "env: XLA_FLAGS=--xla_gpu_deterministic_reductions\n",
      "env: TF_CUDNN_DETERMINISTIC=1\n",
      "env: TF_DETERMINISTIC_OPS=1\n",
      "env: PYTHONHASHSEED=0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:150% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SET ENVIRONMENT VARIABLES\n",
    "%env TF_CPP_MIN_LOG_LEVEL=3\n",
    "%env XLA_FLAGS=--xla_gpu_deterministic_reductions\n",
    "%env TF_CUDNN_DETERMINISTIC=1\n",
    "%env TF_DETERMINISTIC_OPS=1\n",
    "%env PYTHONHASHSEED=0\n",
    "# %env XLA_PYTHON_CLIENT_MEM_FRACTION=0.95  # can help avoid OOM errors\n",
    "\n",
    "# Load modules\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set Jupyter notebook width to display training progress correctly\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:150% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guide to reading the printouts below:\n",
    "\n",
    "### OOD uncertainty estimation metrics:\n",
    "- For all of the below: the first entry is for MNIST, the second entry is for NotMNIST, and the third entry is for KMNIST\n",
    "- `auc ent`: OOD detection AUROC based on predictive entropy\n",
    "- `auc alea`: OOD detection AUROC based on aleatoric uncertainty only\n",
    "- `auc ent`: OOD detection AUROC based on epistemic uncertainty only\n",
    "\n",
    "### Test evaluation metrics:\n",
    "- `te_nll`: Test negative log-likelihood\n",
    "- `te_acc`: Test accuracy\n",
    "- `te ece`: Test expected calibration error (ECE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. FSVI Best Overall: (1) Full covariance in KL + (2) dropout (otherwise same as baseline below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making GPU operations deterministic by setting os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_deterministic_reductions\"\"and os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
      "WARNING: UCI trainer could not be loaded\n",
      "WARNING: Offline RL trainer could not be loaded\n",
      "WARNING: Offline RL evaluator could not be loaded\n",
      "WARNING: Continual learning trainer could not be loaded\n",
      "\n",
      "Device: gpu\n",
      "\n",
      "Input arguments:\n",
      " {\n",
      "    \"data_training\":\"fashionmnist\",\n",
      "    \"data_ood\":[\n",
      "        \"mnist\",\n",
      "        \"notmnist\",\n",
      "        \"kmnist\"\n",
      "    ],\n",
      "    \"optimizer\":\"adam\",\n",
      "    \"optimizer_var\":\"not_specified\",\n",
      "    \"momentum\":0.0,\n",
      "    \"momentum_var\":0.0,\n",
      "    \"schedule\":\"not_specified\",\n",
      "    \"architecture\":\"four_layers\",\n",
      "    \"activation\":\"relu\",\n",
      "    \"prior_mean\":\"0\",\n",
      "    \"prior_cov\":\"0.006\",\n",
      "    \"prior_covs\":[\n",
      "        0.0\n",
      "    ],\n",
      "    \"prior_type\":\"bnn_induced\",\n",
      "    \"epochs\":30,\n",
      "    \"start_var_opt\":0,\n",
      "    \"batch_size\":128,\n",
      "    \"learning_rate\":0.0005,\n",
      "    \"learning_rate_var\":0.001,\n",
      "    \"dropout_rate\":0.1,\n",
      "    \"regularization\":0,\n",
      "    \"n_marginals\":1,\n",
      "    \"n_condition\":128,\n",
      "    \"inducing_input_type\":\"ood_rand\",\n",
      "    \"inducing_input_ood_data\":[\n",
      "        \"kmnist\"\n",
      "    ],\n",
      "    \"inducing_input_ood_data_size\":50000,\n",
      "    \"kl_scale\":\"none\",\n",
      "    \"feature_map_jacobian\":false,\n",
      "    \"feature_map_jacobian_train_only\":false,\n",
      "    \"feature_map_type\":\"not_specified\",\n",
      "    \"td_prior_scale\":4.0,\n",
      "    \"feature_update\":1,\n",
      "    \"full_cov\":true,\n",
      "    \"n_samples\":5,\n",
      "    \"n_samples_eval\":0,\n",
      "    \"tau\":1,\n",
      "    \"noise_std\":1,\n",
      "    \"logging_frequency\":500,\n",
      "    \"figsize\":[\n",
      "        10,\n",
      "        4\n",
      "    ],\n",
      "    \"seed\":0,\n",
      "    \"save_path\":\"results/tmp/fashionmnist_['mnist', 'notmnist', 'kmnist']/model_fsvi_cnn_dropout__architecture_four_layers__priormean_0__priorcov_0.006__optimizer_adam__lr_0.0005__bs_128__indpoints_4__indtype_ood_rand__klscale_none__nsamples_5__nmarginal_1tau_1__indlim_-1.0_1.0__reg_0__seed_0__2\",\n",
      "    \"save\":true,\n",
      "    \"name\":\"\",\n",
      "    \"evaluate\":false,\n",
      "    \"resume_training\":false,\n",
      "    \"no_final_layer_bias\":false,\n",
      "    \"extra_linear_layer\":false,\n",
      "    \"map_initialization\":false,\n",
      "    \"stochastic_linearization\":false,\n",
      "    \"grad_flow_jacobian\":false,\n",
      "    \"stochastic_prior_mean\":\"not_specified\",\n",
      "    \"batch_normalization\":false,\n",
      "    \"batch_normalization_mod\":\"not_specified\",\n",
      "    \"final_layer_variational\":false,\n",
      "    \"kl_sup\":\"max\",\n",
      "    \"kl_sampled\":false,\n",
      "    \"fixed_inner_layers_variational_var\":false,\n",
      "    \"init_logvar\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"init_logvar_lin\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"init_logvar_conv\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"perturbation_param\":0.01,\n",
      "    \"debug\":true,\n",
      "    \"logroot\":null,\n",
      "    \"subdir\":null,\n",
      "    \"wandb_project\":\"not_specified\",\n",
      "    \"task\":\"fashionmnist_['mnist', 'notmnist', 'kmnist']\",\n",
      "    \"init_logvar_minval\":0.0,\n",
      "    \"init_logvar_maxval\":0.0,\n",
      "    \"init_logvar_lin_minval\":0.0,\n",
      "    \"init_logvar_lin_maxval\":0.0,\n",
      "    \"init_logvar_conv_minval\":0.0,\n",
      "    \"init_logvar_conv_maxval\":0.0,\n",
      "    \"model_type\":\"fsvi_cnn_dropout\",\n",
      "    \"n_inducing_inputs\":4,\n",
      "    \"inducing_inputs_bound\":[\n",
      "        -1.0,\n",
      "        1.0\n",
      "    ]\n",
      "} \n",
      "\n",
      "\n",
      "MAP initialization: False\n",
      "Full NTK computation: True\n",
      "Variational final layer: False\n",
      "Stochastic linearization (posterior): False\n",
      "Stochastic linearization (prior): True\n",
      "Gradient flow through Jacobian evaluation: False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "n_samples_eval: 10\n",
      "n_samples_train: 1\n",
      "n_batches_test: 10\n",
      "n_batches_ood: [10, 10, 10]\n",
      "n_eval: 1000\n",
      "ece_over_samples: False\n",
      "\n",
      "--- Training for 30 epochs ---\n",
      "\n",
      "----  --------  --------  --------  --------  --------  -------------------------------  ---------------------------  ---------------------------  ---------  -------------------------------  --------  --------  --------  ---------  -----------\n",
      "  ep    tr_nll    tr_acc    te_nll    te_acc    te ece  auc ent                          auc alea                     auc epis                       te_entr  ood_entr                             elbo      ellk        kl    time_ep    time_eval\n",
      "----  --------  --------  --------  --------  --------  -------------------------------  ---------------------------  ---------------------------  ---------  -------------------------------  --------  --------  --------  ---------  -----------\n",
      "   1     0.389    85.740     0.438    85.710     0.015  |   96.061   93.502   99.610  |  87.833   88.913   94.320  |  99.051   95.742   99.917  |      0.423  |    1.651    1.595    1.923  |     0.000     0.000     0.000     69.272        8.519\n",
      "   2     0.330    87.580     0.369    87.550     0.013  |   98.173   95.156   99.854  |  92.197   90.628   94.664  |  99.561   97.075   99.970  |      0.357  |    1.907    1.681    2.072  |     0.000     0.000     0.000     55.650        3.483\n",
      "   3     0.344    87.440     0.378    87.580     0.014  |   99.350   96.309   99.941  |  94.922   92.169   95.087  |  99.876   97.458   99.987  |      0.295  |    1.939    1.718    2.036  |     0.000     0.000     0.000     55.863        3.356\n",
      "   4     0.245    90.480     0.302    89.680     0.008  |   98.800   96.488   99.961  |  93.812   93.170   95.810  |  99.693   97.451   99.989  |      0.279  |    1.918    1.728    2.089  |     0.000     0.000     0.000     55.982        3.350\n",
      "   5     0.231    91.320     0.294    90.360     0.004  |   98.694   96.029   99.942  |  94.267   92.867   96.146  |  99.611   97.264   99.994  |      0.258  |    1.874    1.693    2.059  |     0.000     0.000     0.000     55.954        3.378\n",
      "   6     0.238    90.850     0.291    90.110     0.008  |   98.365   97.277   99.954  |  93.224   94.397   95.796  |  99.458   97.951   99.995  |      0.254  |    1.845    1.761    2.040  |     0.000     0.000     0.000     55.832        3.411\n",
      "   7     0.197    92.840     0.267    91.090     0.003  |   98.890   97.343   99.982  |  94.976   94.948   96.578  |  99.527   97.983   99.997  |      0.228  |    1.890    1.768    2.057  |     0.000     0.000     0.000     55.843        3.378\n",
      "   8     0.188    92.710     0.265    90.970     0.008  |   99.503   97.226   99.972  |  95.268   94.265   96.200  |  99.845   97.965   99.998  |      0.241  |    1.955    1.758    2.063  |     0.000     0.000     0.000     55.887        3.361\n",
      "   9     0.165    93.830     0.261    91.420     0.005  |   99.476   97.055   99.947  |  94.957   93.728   95.217  |  99.847   98.013   99.997  |      0.228  |    1.962    1.746    2.055  |     0.000     0.000     0.000     55.894        3.390\n",
      "  10     0.150    94.360     0.256    92.030     0.005  |   98.855   96.892   99.974  |  94.305   93.951   96.079  |  99.581   97.774   99.998  |      0.224  |    1.860    1.713    2.039  |     0.000     0.000     0.000     55.865        3.354\n",
      "  11     0.147    94.340     0.246    92.020     0.003  |   99.214   96.812   99.956  |  95.573   94.293   96.536  |  99.727   97.732   99.999  |      0.212  |    1.927    1.725    2.060  |     0.000     0.000     0.000     55.878        3.419\n",
      "  12     0.127    95.200     0.251    92.310     0.005  |   98.847   95.971   99.980  |  96.213   93.522   97.241  |  99.440   96.977   99.999  |      0.193  |    1.920    1.673    2.053  |     0.000     0.000     0.000     55.865        3.422\n",
      "  13     0.121    95.480     0.258    92.370     0.009  |   99.378   97.234   99.960  |  96.771   95.030   97.660  |  99.779   98.020   99.999  |      0.187  |    1.943    1.696    2.069  |     0.000     0.000     0.000     55.899        3.388\n",
      "  14     0.117    95.480     0.257    92.290     0.008  |   99.514   96.891   99.959  |  97.404   95.063   97.959  |  99.801   97.717   99.999  |      0.183  |    1.971    1.713    2.057  |     0.000     0.000     0.000     55.901        3.441\n",
      "  15     0.090    96.520     0.258    92.560     0.008  |   99.200   96.750   99.961  |  97.119   94.759   97.747  |  99.608   97.581   99.999  |      0.175  |    1.923    1.689    2.045  |     0.000     0.000     0.000     55.924        3.362\n",
      "  16     0.093    96.600     0.284    92.440     0.016  |   98.955   96.736   99.984  |  97.088   95.129   98.589  |  99.422   97.408   99.999  |      0.153  |    1.856    1.637    2.033  |     0.000     0.000     0.000     55.852        3.422\n",
      "  17     0.081    97.150     0.273    92.380     0.013  |   99.603   97.436   99.945  |  97.858   95.965   98.335  |  99.867   98.029   99.998  |      0.163  |    1.928    1.724    2.028  |     0.000     0.000     0.000     55.887        3.367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  18     0.075    97.290     0.280    92.360     0.015  |   99.305   97.196   99.972  |  97.469   95.704   98.578  |  99.630   97.836   99.999  |      0.158  |    1.908    1.660    2.047  |     0.000     0.000     0.000     56.007        3.357\n",
      "  19     0.077    96.910     0.301    92.330     0.021  |   98.959   97.161   99.977  |  97.564   95.849   99.034  |  99.346   97.637   99.998  |      0.146  |    1.869    1.670    2.041  |     0.000     0.000     0.000     55.971        3.393\n",
      "  20     0.063    97.660     0.291    92.690     0.015  |   99.431   97.039   99.974  |  98.193   95.534   98.761  |  99.689   97.652   99.999  |      0.149  |    1.901    1.664    2.018  |     0.000     0.000     0.000     56.034        3.408\n",
      "  21     0.068    97.290     0.316    92.270     0.021  |   99.623   97.365   99.930  |  98.619   96.324   99.023  |  99.840   97.839   99.999  |      0.146  |    1.933    1.708    2.015  |     0.000     0.000     0.000     56.012        3.381\n",
      "  22     0.042    98.520     0.311    92.580     0.021  |   99.444   97.493   99.963  |  98.369   96.352   99.200  |  99.693   98.000   99.998  |      0.136  |    1.895    1.646    2.031  |     0.000     0.000     0.000     55.986        3.398\n",
      "  23     0.058    97.860     0.326    92.590     0.018  |   99.666   97.677   99.974  |  98.877   96.559   99.237  |  99.819   98.209   99.997  |      0.141  |    1.939    1.696    2.028  |     0.000     0.000     0.000     55.979        3.394\n",
      "  24     0.037    98.810     0.328    92.850     0.021  |   99.555   97.775   99.964  |  98.785   96.840   99.365  |  99.752   98.126   99.998  |      0.128  |    1.927    1.679    2.033  |     0.000     0.000     0.000     56.045        3.419\n",
      "  25     0.037    98.770     0.338    92.930     0.022  |   99.602   97.553   99.977  |  98.907   96.664   99.361  |  99.750   97.816   99.997  |      0.123  |    1.905    1.659    2.013  |     0.000     0.000     0.000     55.948        3.363\n",
      "  26     0.036    98.620     0.348    92.920     0.021  |   99.602   97.324   99.994  |  98.958   96.442   99.427  |  99.730   97.627   99.998  |      0.124  |    1.928    1.633    2.006  |     0.000     0.000     0.000     56.017        3.396\n",
      "  27     0.035    98.690     0.368    92.490     0.027  |   99.567   97.507   99.985  |  99.024   96.716   99.556  |  99.704   97.765   99.996  |      0.120  |    1.952    1.678    2.054  |     0.000     0.000     0.000     55.881        3.414\n",
      "  28     0.029    98.860     0.380    92.520     0.028  |   99.425   97.883   99.972  |  98.739   97.172   99.504  |  99.604   98.085   99.998  |      0.117  |    1.900    1.684    2.023  |     0.000     0.000     0.000     55.947        3.415\n",
      "  29     0.022    99.180     0.385    92.890     0.026  |   99.692   97.953   99.970  |  99.224   97.293   99.590  |  99.809   98.142   99.997  |      0.114  |    1.945    1.692    2.037  |     0.000     0.000     0.000     55.890        3.405\n",
      "  30     0.026    99.080     0.402    92.880     0.028  |   99.331   97.817   99.975  |  98.923   97.127   99.687  |  99.457   97.957   99.996  |      0.108  |    1.909    1.635    2.030  |     0.000     0.000     0.000     55.893        3.410\n",
      "\n",
      "Changing save path from\n",
      "\n",
      "results/tmp/fashionmnist_['mnist', 'notmnist', 'kmnist']/model_fsvi_cnn_dropout__architecture_four_layers__priormean_0__priorcov_0.006__optimizer_adam__lr_0.0005__bs_128__indpoints_4__indtype_ood_rand__klscale_none__nsamples_5__nmarginal_1tau_1__indlim_-1.0_1.0__reg_0__seed_0__2\n",
      "\n",
      "to\n",
      "\n",
      "results/tmp/fashionmnist_['mnist', 'notmnist', 'kmnist']/model_fsvi_cnn_dropout__architecture_four_layers__priormean_0__priorcov_0.006__optimizer_adam__lr_0.0005__bs_128__indpoints_4__indtype_ood_rand__klscale_none__nsamples_5__nmarginal_1tau_1__indlim_-1.0_1.0__reg_0__seed_0__2__complete\n",
      "\n",
      "\n",
      "------------------- DONE -------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = os.path.abspath(os.path.dirname(os.getcwd()))\n",
    "\n",
    "if os.getcwd()[-4:] == 'fsvi':\n",
    "    pass\n",
    "else:\n",
    "    os.chdir(path)\n",
    "\n",
    "%run run_base.py\\\n",
    "--data_training fashionmnist --data_ood mnist notmnist kmnist --model fsvi_cnn_dropout --architecture four_layers --activation relu --epochs 30 --learning_rate 5e-4 --optimizer adam --batch_size 128 --prior_mean 0 --prior_cov 0.006 --prior_type bnn_induced --inducing_points 4 --n_marginals 1 --kl_sup max --inducing_input_type ood_rand --inducing_input_ood_data kmnist --kl_scale none --n_samples 5 --logging_frequency 500 --seed 0 --debug --save --save_path tmp --feature_update 1 --full_cov --dropout_rate 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. FSVI Baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making GPU operations deterministic by setting os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_deterministic_reductions\"\"and os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
      "WARNING: UCI trainer could not be loaded\n",
      "WARNING: Offline RL trainer could not be loaded\n",
      "WARNING: Offline RL evaluator could not be loaded\n",
      "WARNING: Continual learning trainer could not be loaded\n",
      "\n",
      "Device: gpu\n",
      "\n",
      "Input arguments:\n",
      " {\n",
      "    \"data_training\":\"fashionmnist\",\n",
      "    \"data_ood\":[\n",
      "        \"mnist\",\n",
      "        \"notmnist\",\n",
      "        \"kmnist\"\n",
      "    ],\n",
      "    \"optimizer\":\"adam\",\n",
      "    \"optimizer_var\":\"not_specified\",\n",
      "    \"momentum\":0.0,\n",
      "    \"momentum_var\":0.0,\n",
      "    \"schedule\":\"not_specified\",\n",
      "    \"architecture\":\"four_layers\",\n",
      "    \"activation\":\"relu\",\n",
      "    \"prior_mean\":\"0\",\n",
      "    \"prior_cov\":\"10\",\n",
      "    \"prior_covs\":[\n",
      "        0.0\n",
      "    ],\n",
      "    \"prior_type\":\"fixed\",\n",
      "    \"epochs\":30,\n",
      "    \"start_var_opt\":0,\n",
      "    \"batch_size\":128,\n",
      "    \"learning_rate\":0.0005,\n",
      "    \"learning_rate_var\":0.001,\n",
      "    \"dropout_rate\":0.0,\n",
      "    \"regularization\":0,\n",
      "    \"n_marginals\":1,\n",
      "    \"n_condition\":128,\n",
      "    \"inducing_input_type\":\"ood_rand\",\n",
      "    \"inducing_input_ood_data\":[\n",
      "        \"kmnist\"\n",
      "    ],\n",
      "    \"inducing_input_ood_data_size\":50000,\n",
      "    \"kl_scale\":\"none\",\n",
      "    \"feature_map_jacobian\":false,\n",
      "    \"feature_map_jacobian_train_only\":false,\n",
      "    \"feature_map_type\":\"not_specified\",\n",
      "    \"td_prior_scale\":4.0,\n",
      "    \"feature_update\":1,\n",
      "    \"full_cov\":false,\n",
      "    \"n_samples\":5,\n",
      "    \"n_samples_eval\":0,\n",
      "    \"tau\":1,\n",
      "    \"noise_std\":1,\n",
      "    \"logging_frequency\":500,\n",
      "    \"figsize\":[\n",
      "        10,\n",
      "        4\n",
      "    ],\n",
      "    \"seed\":0,\n",
      "    \"save_path\":\"results/tmp/fashionmnist_['mnist', 'notmnist', 'kmnist']/model_fsvi_cnn__architecture_four_layers__priormean_0__priorcov_10__optimizer_adam__lr_0.0005__bs_128__indpoints_4__indtype_ood_rand__klscale_none__nsamples_5__nmarginal_1tau_1__indlim_-1.0_1.0__reg_0__seed_0__2\",\n",
      "    \"save\":true,\n",
      "    \"name\":\"\",\n",
      "    \"evaluate\":false,\n",
      "    \"resume_training\":false,\n",
      "    \"final_layer_bias\":false,\n",
      "    \"extra_linear_layer\":false,\n",
      "    \"map_initialization\":false,\n",
      "    \"stochastic_linearization\":false,\n",
      "    \"grad_flow_jacobian\":false,\n",
      "    \"stochastic_prior_mean\":\"not_specified\",\n",
      "    \"batch_normalization\":false,\n",
      "    \"batch_normalization_mod\":\"not_specified\",\n",
      "    \"final_layer_variational\":false,\n",
      "    \"kl_sup\":\"max\",\n",
      "    \"kl_sampled\":false,\n",
      "    \"fixed_inner_layers_variational_var\":false,\n",
      "    \"init_logvar\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"init_logvar_lin\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"init_logvar_conv\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"perturbation_param\":0.01,\n",
      "    \"debug\":true,\n",
      "    \"logroot\":null,\n",
      "    \"subdir\":null,\n",
      "    \"wandb_project\":\"not_specified\",\n",
      "    \"task\":\"fashionmnist_['mnist', 'notmnist', 'kmnist']\",\n",
      "    \"init_logvar_minval\":0.0,\n",
      "    \"init_logvar_maxval\":0.0,\n",
      "    \"init_logvar_lin_minval\":0.0,\n",
      "    \"init_logvar_lin_maxval\":0.0,\n",
      "    \"init_logvar_conv_minval\":0.0,\n",
      "    \"init_logvar_conv_maxval\":0.0,\n",
      "    \"model_type\":\"fsvi_cnn\",\n",
      "    \"n_inducing_inputs\":4,\n",
      "    \"inducing_inputs_bound\":[\n",
      "        -1.0,\n",
      "        1.0\n",
      "    ]\n",
      "} \n",
      "\n",
      "\n",
      "MAP initialization: False\n",
      "Full NTK computation: False\n",
      "Variational final layer: False\n",
      "Stochastic linearization (posterior): False\n",
      "Stochastic linearization (prior): False\n",
      "Gradient flow through Jacobian evaluation: False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "n_samples_eval: 10\n",
      "n_samples_train: 1\n",
      "n_batches_test: 10\n",
      "n_batches_ood: [10, 10, 10]\n",
      "n_eval: 1000\n",
      "ece_over_samples: False\n",
      "\n",
      "--- Training for 30 epochs ---\n",
      "\n",
      "----  --------  --------  --------  --------  --------  -------------------------------  ---------------------------  ---------------------------  ---------  -------------------------------  --------  --------  --------  ---------  -----------\n",
      "  ep    tr_nll    tr_acc    te_nll    te_acc    te ece  auc ent                          auc alea                     auc epis                       te_entr  ood_entr                             elbo      ellk        kl    time_ep    time_eval\n",
      "----  --------  --------  --------  --------  --------  -------------------------------  ---------------------------  ---------------------------  ---------  -------------------------------  --------  --------  --------  ---------  -----------\n",
      "   1     0.325    88.620     0.352    87.690     0.015  |   95.842   93.968   99.509  |  95.919   94.027   99.516  |  79.937   83.958   86.218  |      0.385  |    1.683    1.561    2.028  |     0.000     0.000     0.000     25.301        6.229\n",
      "   2     0.266    89.930     0.301    89.310     0.007  |   97.733   96.103   99.810  |  97.768   96.124   99.812  |  74.405   83.987   73.840  |      0.310  |    1.889    1.663    2.185  |     0.000     0.000     0.000     19.616        1.726\n",
      "   3     0.286    89.650     0.311    89.090     0.019  |   98.814   94.848   99.902  |  98.845   94.899   99.906  |  74.072   82.266   71.651  |      0.243  |    1.985    1.547    2.182  |     0.000     0.000     0.000     19.634        1.731\n",
      "   4     0.194    92.670     0.257    91.070     0.006  |   99.044   95.262   99.961  |  99.065   95.289   99.963  |  73.608   84.158   67.216  |      0.236  |    2.031    1.541    2.240  |     0.000     0.000     0.000     19.627        1.732\n",
      "   5     0.188    93.260     0.264    90.540     0.013  |   99.148   94.334   99.976  |  99.170   94.357   99.976  |  74.097   84.195   67.858  |      0.220  |    2.059    1.482    2.250  |     0.000     0.000     0.000     19.626        1.728\n",
      "   6     0.185    92.880     0.257    91.160     0.014  |   98.319   94.306   99.934  |  98.356   94.346   99.937  |  76.393   84.787   70.495  |      0.197  |    1.956    1.435    2.224  |     0.000     0.000     0.000     19.629        1.726\n",
      "   7     0.146    94.750     0.238    91.690     0.013  |   99.462   95.831   99.988  |  99.480   95.847   99.989  |  76.214   85.728   69.828  |      0.186  |    2.096    1.601    2.258  |     0.000     0.000     0.000     19.594        1.727\n",
      "   8     0.135    94.900     0.244    91.490     0.020  |   99.698   94.521   99.995  |  99.709   94.529   99.995  |  76.428   86.340   69.480  |      0.177  |    2.138    1.464    2.269  |     0.000     0.000     0.000     19.616        1.717\n",
      "   9     0.100    96.460     0.237    92.040     0.019  |   99.791   94.939   99.998  |  99.800   94.951   99.998  |  78.317   86.878   68.513  |      0.162  |    2.143    1.447    2.274  |     0.000     0.000     0.000     19.598        1.751\n",
      "  10     0.104    96.240     0.253    92.020     0.023  |   99.732   94.042   99.996  |  99.745   94.053   99.996  |  79.595   87.877   72.382  |      0.154  |    2.123    1.328    2.261  |     0.000     0.000     0.000     19.597        1.730\n",
      "  11     0.083    97.080     0.249    92.280     0.023  |   99.532   94.845   99.996  |  99.551   94.881   99.996  |  79.413   87.395   66.701  |      0.142  |    2.087    1.404    2.274  |     0.000     0.000     0.000     19.549        1.724\n",
      "  12     0.082    96.990     0.267    92.350     0.030  |   99.361   94.459   99.996  |  99.395   94.529   99.997  |  81.287   87.412   70.053  |      0.121  |    2.039    1.292    2.272  |     0.000     0.000     0.000     19.546        1.737\n",
      "  13     0.059    98.010     0.262    92.400     0.028  |   99.824   93.976   99.999  |  99.835   94.014   99.999  |  80.671   87.379   69.506  |      0.127  |    2.148    1.315    2.278  |     0.000     0.000     0.000     19.675        1.765\n",
      "  14     0.060    97.860     0.282    92.170     0.032  |   99.885   95.634   99.999  |  99.895   95.698   99.999  |  77.125   87.257   62.049  |      0.122  |    2.177    1.407    2.286  |     0.000     0.000     0.000     19.723        1.723\n",
      "  15     0.037    98.710     0.284    92.300     0.035  |   99.760   94.853   99.999  |  99.778   94.914   99.999  |  81.132   88.284   66.509  |      0.109  |    2.138    1.319    2.281  |     0.000     0.000     0.000     19.721        1.730\n",
      "  16     0.034    98.870     0.319    92.180     0.039  |   99.682   95.478   99.999  |  99.711   95.572   99.999  |  82.883   88.646   63.726  |      0.099  |    2.097    1.319    2.283  |     0.000     0.000     0.000     19.703        1.730\n",
      "  17     0.042    98.520     0.343    91.750     0.044  |   99.820   94.646   99.998  |  99.840   94.749   99.999  |  81.501   87.941   65.632  |      0.097  |    2.125    1.269    2.278  |     0.000     0.000     0.000     19.723        1.715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  18     0.025    98.980     0.356    92.310     0.043  |   99.690   95.582   99.998  |  99.721   95.697   99.998  |  82.459   88.812   62.150  |      0.086  |    2.090    1.309    2.284  |     0.000     0.000     0.000     19.707        1.734\n",
      "  19     0.042    98.340     0.389    91.850     0.045  |   99.577   95.193   99.999  |  99.626   95.361   99.999  |  82.962   87.782   64.854  |      0.090  |    2.062    1.289    2.278  |     0.000     0.000     0.000     19.709        1.722\n",
      "  20     0.024    99.100     0.376    92.340     0.044  |   99.697   95.410   99.999  |  99.736   95.566   99.999  |  81.640   88.007   62.694  |      0.082  |    2.103    1.303    2.282  |     0.000     0.000     0.000     19.713        1.717\n",
      "  21     0.037    98.530     0.420    92.110     0.046  |   99.794   95.757  100.000  |  99.826   95.939  100.000  |  81.071   87.620   59.260  |      0.081  |    2.139    1.338    2.288  |     0.000     0.000     0.000     19.671        1.737\n",
      "  22     0.010    99.720     0.406    92.090     0.048  |   99.833   95.877  100.000  |  99.856   95.997  100.000  |  79.515   88.384   50.882  |      0.079  |    2.148    1.352    2.293  |     0.000     0.000     0.000     19.727        1.736\n",
      "  23     0.019    99.330     0.414    92.190     0.047  |   99.762   95.911  100.000  |  99.795   96.042  100.000  |  78.018   87.198   45.435  |      0.078  |    2.135    1.430    2.294  |     0.000     0.000     0.000     19.700        1.730\n",
      "  24     0.023    99.210     0.462    91.980     0.049  |   99.895   96.175  100.000  |  99.913   96.347  100.000  |  80.579   88.038   51.541  |      0.075  |    2.157    1.380    2.292  |     0.000     0.000     0.000     19.713        1.726\n",
      "  25     0.010    99.640     0.450    92.550     0.046  |   99.695   95.988  100.000  |  99.741   96.169  100.000  |  81.830   88.708   50.992  |      0.070  |    2.088    1.313    2.293  |     0.000     0.000     0.000     19.712        1.737\n",
      "  26     0.015    99.400     0.478    92.060     0.051  |   99.812   95.844   99.999  |  99.845   96.030  100.000  |  82.094   88.696   52.361  |      0.071  |    2.110    1.278    2.290  |     0.000     0.000     0.000     19.719        1.752\n",
      "  27     0.022    99.250     0.485    91.970     0.051  |   99.763   95.242  100.000  |  99.797   95.404  100.000  |  78.170   87.826   44.428  |      0.072  |    2.128    1.277    2.294  |     0.000     0.000     0.000     19.726        1.724\n",
      "  28     0.010    99.720     0.482    92.270     0.049  |   99.857   95.543  100.000  |  99.882   95.733  100.000  |  76.681   87.119   42.761  |      0.070  |    2.153    1.287    2.295  |     0.000     0.000     0.000     19.731        1.727\n",
      "  29     0.005    99.820     0.500    92.420     0.049  |   99.836   95.464  100.000  |  99.868   95.666  100.000  |  80.992   87.961   50.828  |      0.065  |    2.120    1.242    2.290  |     0.000     0.000     0.000     19.710        1.747\n",
      "  30     0.010    99.650     0.545    92.410     0.051  |   99.750   95.098  100.000  |  99.796   95.333  100.000  |  83.154   87.821   53.337  |      0.061  |    2.092    1.202    2.291  |     0.000     0.000     0.000     19.710        1.747\n",
      "\n",
      "Changing save path from\n",
      "\n",
      "results/tmp/fashionmnist_['mnist', 'notmnist', 'kmnist']/model_fsvi_cnn__architecture_four_layers__priormean_0__priorcov_10__optimizer_adam__lr_0.0005__bs_128__indpoints_4__indtype_ood_rand__klscale_none__nsamples_5__nmarginal_1tau_1__indlim_-1.0_1.0__reg_0__seed_0__2\n",
      "\n",
      "to\n",
      "\n",
      "results/tmp/fashionmnist_['mnist', 'notmnist', 'kmnist']/model_fsvi_cnn__architecture_four_layers__priormean_0__priorcov_10__optimizer_adam__lr_0.0005__bs_128__indpoints_4__indtype_ood_rand__klscale_none__nsamples_5__nmarginal_1tau_1__indlim_-1.0_1.0__reg_0__seed_0__2__complete\n",
      "\n",
      "\n",
      "------------------- DONE -------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = os.path.abspath(os.path.dirname(os.getcwd()))\n",
    "\n",
    "if os.getcwd()[-4:] == 'fsvi':\n",
    "    pass\n",
    "else:\n",
    "    os.chdir(path)\n",
    "\n",
    "%run run_base.py\\\n",
    "--data_training fashionmnist --data_ood mnist notmnist kmnist --model fsvi_cnn --architecture four_layers --activation relu --epochs 30 --learning_rate 5e-4 --optimizer adam --batch_size 128 --prior_mean 0 --prior_cov 10 --prior_type fixed --inducing_points 4 --n_marginals 1 --kl_sup max --inducing_input_type ood_rand --inducing_input_ood_data kmnist --kl_scale none --n_samples 5 --logging_frequency 500 --seed 0 --debug --save --save_path tmp --feature_update 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Full covariance in KL (otherwise same as baseline):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making GPU operations deterministic by setting os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_deterministic_reductions\"\"and os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
      "WARNING: UCI trainer could not be loaded\n",
      "WARNING: Offline RL trainer could not be loaded\n",
      "WARNING: Offline RL evaluator could not be loaded\n",
      "WARNING: Continual learning trainer could not be loaded\n",
      "\n",
      "Device: gpu\n",
      "\n",
      "Input arguments:\n",
      " {\n",
      "    \"data_training\":\"fashionmnist\",\n",
      "    \"data_ood\":[\n",
      "        \"mnist\",\n",
      "        \"notmnist\",\n",
      "        \"kmnist\"\n",
      "    ],\n",
      "    \"optimizer\":\"adam\",\n",
      "    \"optimizer_var\":\"not_specified\",\n",
      "    \"momentum\":0.0,\n",
      "    \"momentum_var\":0.0,\n",
      "    \"schedule\":\"not_specified\",\n",
      "    \"architecture\":\"four_layers\",\n",
      "    \"activation\":\"relu\",\n",
      "    \"prior_mean\":\"0\",\n",
      "    \"prior_cov\":\"0.005\",\n",
      "    \"prior_covs\":[\n",
      "        0.0\n",
      "    ],\n",
      "    \"prior_type\":\"bnn_induced\",\n",
      "    \"epochs\":30,\n",
      "    \"start_var_opt\":0,\n",
      "    \"batch_size\":128,\n",
      "    \"learning_rate\":0.0005,\n",
      "    \"learning_rate_var\":0.001,\n",
      "    \"dropout_rate\":0.0,\n",
      "    \"regularization\":0,\n",
      "    \"n_marginals\":1,\n",
      "    \"n_condition\":128,\n",
      "    \"inducing_input_type\":\"ood_rand\",\n",
      "    \"inducing_input_ood_data\":[\n",
      "        \"kmnist\"\n",
      "    ],\n",
      "    \"inducing_input_ood_data_size\":50000,\n",
      "    \"kl_scale\":\"none\",\n",
      "    \"feature_map_jacobian\":false,\n",
      "    \"feature_map_jacobian_train_only\":false,\n",
      "    \"feature_map_type\":\"not_specified\",\n",
      "    \"td_prior_scale\":4.0,\n",
      "    \"feature_update\":1,\n",
      "    \"full_cov\":true,\n",
      "    \"n_samples\":5,\n",
      "    \"n_samples_eval\":0,\n",
      "    \"tau\":1,\n",
      "    \"noise_std\":1,\n",
      "    \"logging_frequency\":500,\n",
      "    \"figsize\":[\n",
      "        10,\n",
      "        4\n",
      "    ],\n",
      "    \"seed\":0,\n",
      "    \"save_path\":\"results/tmp/fashionmnist_['mnist', 'notmnist', 'kmnist']/model_fsvi_cnn__architecture_four_layers__priormean_0__priorcov_0.005__optimizer_adam__lr_0.0005__bs_128__indpoints_4__indtype_ood_rand__klscale_none__nsamples_5__nmarginal_1tau_1__indlim_-1.0_1.0__reg_0__seed_0__1\",\n",
      "    \"save\":true,\n",
      "    \"name\":\"\",\n",
      "    \"evaluate\":false,\n",
      "    \"resume_training\":false,\n",
      "    \"final_layer_bias\":false,\n",
      "    \"extra_linear_layer\":false,\n",
      "    \"map_initialization\":false,\n",
      "    \"stochastic_linearization\":false,\n",
      "    \"grad_flow_jacobian\":false,\n",
      "    \"stochastic_prior_mean\":\"not_specified\",\n",
      "    \"batch_normalization\":false,\n",
      "    \"batch_normalization_mod\":\"not_specified\",\n",
      "    \"final_layer_variational\":false,\n",
      "    \"kl_sup\":\"max\",\n",
      "    \"kl_sampled\":false,\n",
      "    \"fixed_inner_layers_variational_var\":false,\n",
      "    \"init_logvar\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"init_logvar_lin\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"init_logvar_conv\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"perturbation_param\":0.01,\n",
      "    \"debug\":true,\n",
      "    \"logroot\":null,\n",
      "    \"subdir\":null,\n",
      "    \"wandb_project\":\"not_specified\",\n",
      "    \"task\":\"fashionmnist_['mnist', 'notmnist', 'kmnist']\",\n",
      "    \"init_logvar_minval\":0.0,\n",
      "    \"init_logvar_maxval\":0.0,\n",
      "    \"init_logvar_lin_minval\":0.0,\n",
      "    \"init_logvar_lin_maxval\":0.0,\n",
      "    \"init_logvar_conv_minval\":0.0,\n",
      "    \"init_logvar_conv_maxval\":0.0,\n",
      "    \"model_type\":\"fsvi_cnn\",\n",
      "    \"n_inducing_inputs\":4,\n",
      "    \"inducing_inputs_bound\":[\n",
      "        -1.0,\n",
      "        1.0\n",
      "    ]\n",
      "} \n",
      "\n",
      "\n",
      "MAP initialization: False\n",
      "Full NTK computation: True\n",
      "Variational final layer: False\n",
      "Stochastic linearization (posterior): False\n",
      "Stochastic linearization (prior): True\n",
      "Gradient flow through Jacobian evaluation: False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "n_samples_eval: 10\n",
      "n_samples_train: 1\n",
      "n_batches_test: 10\n",
      "n_batches_ood: [10, 10, 10]\n",
      "n_eval: 1000\n",
      "ece_over_samples: False\n",
      "\n",
      "--- Training for 30 epochs ---\n",
      "\n",
      "----  --------  --------  --------  --------  --------  -------------------------------  ---------------------------  ---------------------------  ---------  -------------------------------  --------  --------  --------  ---------  -----------\n",
      "  ep    tr_nll    tr_acc    te_nll    te_acc    te ece  auc ent                          auc alea                     auc epis                       te_entr  ood_entr                             elbo      ellk        kl    time_ep    time_eval\n",
      "----  --------  --------  --------  --------  --------  -------------------------------  ---------------------------  ---------------------------  ---------  -------------------------------  --------  --------  --------  ---------  -----------\n",
      "   1     0.366    87.150     0.394    86.610     0.015  |   97.995   93.571   99.816  |  97.221   91.692   99.079  |  99.331   95.994   99.946  |      0.408  |    1.806    1.527    2.031  |     0.000     0.000     0.000     64.555        5.687\n",
      "   2     0.311    88.640     0.348    87.940     0.009  |   99.182   96.033   99.912  |  98.456   94.121   98.780  |  99.715   97.627   99.985  |      0.346  |    2.103    1.694    2.147  |     0.000     0.000     0.000     53.950        1.798\n",
      "   3     0.310    88.890     0.347    88.340     0.018  |   99.903   95.600   99.901  |  99.401   94.125   98.932  |  99.980   96.961   99.991  |      0.272  |    2.146    1.639    2.073  |     0.000     0.000     0.000     53.825        1.771\n",
      "   4     0.220    91.500     0.280    90.290     0.004  |   99.823   96.691   99.952  |  99.239   95.652   99.146  |  99.962   97.838   99.994  |      0.267  |    2.124    1.729    2.137  |     0.000     0.000     0.000     53.786        1.776\n",
      "   5     0.215    91.980     0.287    89.990     0.009  |   99.767   96.470   99.940  |  99.280   95.510   99.211  |  99.956   97.443   99.995  |      0.247  |    2.145    1.743    2.160  |     0.000     0.000     0.000     53.986        1.774\n",
      "   6     0.202    92.280     0.266    90.670     0.013  |   99.702   97.463   99.966  |  99.334   96.840   99.451  |  99.895   98.473   99.997  |      0.219  |    2.141    1.779    2.163  |     0.000     0.000     0.000     53.779        1.774\n",
      "   7     0.170    93.830     0.250    91.290     0.008  |   99.698   97.839   99.949  |  99.230   97.170   99.318  |  99.913   98.580   99.996  |      0.214  |    2.115    1.842    2.143  |     0.000     0.000     0.000     53.775        1.774\n",
      "   8     0.162    93.750     0.247    91.090     0.010  |   99.878   97.632   99.961  |  99.520   97.068   99.425  |  99.973   98.529   99.997  |      0.210  |    2.167    1.828    2.149  |     0.000     0.000     0.000     53.822        1.799\n",
      "   9     0.133    95.130     0.240    91.680     0.009  |   99.623   97.445   99.950  |  99.073   96.715   99.284  |  99.917   98.542   99.997  |      0.203  |    2.114    1.820    2.154  |     0.000     0.000     0.000     54.006        1.786\n",
      "  10     0.138    94.920     0.247    91.780     0.015  |   99.411   97.389   99.959  |  98.933   96.825   99.451  |  99.812   98.393   99.997  |      0.186  |    2.084    1.810    2.145  |     0.000     0.000     0.000     53.974        1.787\n",
      "  11     0.121    95.700     0.239    92.080     0.014  |   99.748   97.598   99.951  |  99.343   97.136   99.548  |  99.948   98.343   99.997  |      0.175  |    2.122    1.818    2.156  |     0.000     0.000     0.000     54.031        1.779\n",
      "  12     0.113    95.900     0.251    91.960     0.021  |   99.805   96.776   99.982  |  99.565   96.377   99.684  |  99.932   97.519   99.998  |      0.158  |    2.141    1.771    2.161  |     0.000     0.000     0.000     53.981        1.793\n",
      "  13     0.105    96.240     0.252    91.840     0.023  |   99.848   97.566   99.975  |  99.593   97.140   99.658  |  99.949   98.203   99.998  |      0.156  |    2.151    1.825    2.176  |     0.000     0.000     0.000     53.985        1.792\n",
      "  14     0.095    96.470     0.266    91.540     0.027  |   99.881   97.464   99.965  |  99.616   97.074   99.662  |  99.976   98.178   99.999  |      0.151  |    2.131    1.824    2.163  |     0.000     0.000     0.000     54.003        1.788\n",
      "  15     0.073    97.630     0.260    92.240     0.025  |   99.740   97.153   99.992  |  99.485   96.836   99.784  |  99.900   97.815   99.998  |      0.139  |    2.123    1.820    2.186  |     0.000     0.000     0.000     54.022        1.784\n",
      "  16     0.066    97.580     0.289    91.800     0.034  |   99.733   97.602   99.986  |  99.560   97.339   99.847  |  99.893   98.057   99.998  |      0.126  |    2.138    1.795    2.189  |     0.000     0.000     0.000     54.021        1.805\n",
      "  17     0.057    98.200     0.284    92.230     0.032  |   99.780   96.826   99.988  |  99.610   96.558   99.843  |  99.914   97.414   99.998  |      0.122  |    2.107    1.739    2.167  |     0.000     0.000     0.000     53.981        1.796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  18     0.048    98.420     0.302    92.060     0.038  |   99.779   97.340   99.988  |  99.636   97.125   99.869  |  99.894   97.776   99.998  |      0.109  |    2.116    1.780    2.185  |     0.000     0.000     0.000     54.003        1.801\n",
      "  19     0.044    98.450     0.317    92.250     0.038  |   99.848   97.484   99.988  |  99.749   97.274   99.907  |  99.933   97.920   99.999  |      0.100  |    2.115    1.726    2.159  |     0.000     0.000     0.000     54.019        1.789\n",
      "  20     0.036    98.900     0.321    92.170     0.038  |   99.908   97.155   99.987  |  99.816   96.920   99.888  |  99.963   97.641   99.998  |      0.102  |    2.131    1.735    2.161  |     0.000     0.000     0.000     53.988        1.786\n",
      "  21     0.080    96.920     0.378    90.920     0.049  |   99.780   97.164   99.985  |  99.601   96.884   99.816  |  99.890   97.628   99.999  |      0.107  |    2.112    1.729    2.144  |     0.000     0.000     0.000     53.814        1.750\n",
      "  22     0.034    98.750     0.361    92.010     0.044  |   99.855   97.366   99.993  |  99.720   97.125   99.855  |  99.921   97.828   99.999  |      0.094  |    2.150    1.710    2.176  |     0.000     0.000     0.000     53.794        1.770\n",
      "  23     0.033    98.970     0.361    91.940     0.042  |   99.884   97.381   99.961  |  99.770   97.163   99.848  |  99.959   97.803   99.999  |      0.099  |    2.146    1.747    2.157  |     0.000     0.000     0.000     53.796        1.770\n",
      "  24     0.019    99.390     0.361    92.350     0.043  |   99.893   97.943   99.996  |  99.780   97.741   99.867  |  99.940   98.230   99.999  |      0.084  |    2.137    1.768    2.163  |     0.000     0.000     0.000     53.812        1.778\n",
      "  25     0.024    99.150     0.398    91.910     0.049  |   99.909   97.305   99.997  |  99.813   97.105   99.889  |  99.951   97.577  100.000  |      0.082  |    2.135    1.697    2.162  |     0.000     0.000     0.000     53.818        1.775\n",
      "  26     0.019    99.310     0.405    92.020     0.047  |   99.822   97.246  100.000  |  99.718   97.048   99.893  |  99.871   97.471   99.999  |      0.082  |    2.123    1.700    2.172  |     0.000     0.000     0.000     53.812        1.774\n",
      "  27     0.017    99.480     0.422    92.000     0.050  |   99.909   97.491   99.999  |  99.829   97.309   99.915  |  99.945   97.718   99.999  |      0.076  |    2.136    1.679    2.172  |     0.000     0.000     0.000     53.897        1.785\n",
      "  28     0.013    99.570     0.439    91.980     0.052  |   99.862   97.302   99.987  |  99.800   97.158   99.931  |  99.914   97.451   99.999  |      0.070  |    2.101    1.681    2.162  |     0.000     0.000     0.000     54.005        1.786\n",
      "  29     0.009    99.770     0.444    91.900     0.053  |   99.783   97.299   99.985  |  99.666   97.105   99.860  |  99.853   97.465  100.000  |      0.072  |    2.117    1.727    2.171  |     0.000     0.000     0.000     53.991        1.786\n",
      "  30     0.017    99.450     0.446    91.760     0.052  |   99.879   98.072   99.991  |  99.794   97.818   99.857  |  99.915   98.380   99.998  |      0.076  |    2.127    1.735    2.166  |     0.000     0.000     0.000     54.052        1.816\n",
      "\n",
      "Changing save path from\n",
      "\n",
      "results/tmp/fashionmnist_['mnist', 'notmnist', 'kmnist']/model_fsvi_cnn__architecture_four_layers__priormean_0__priorcov_0.005__optimizer_adam__lr_0.0005__bs_128__indpoints_4__indtype_ood_rand__klscale_none__nsamples_5__nmarginal_1tau_1__indlim_-1.0_1.0__reg_0__seed_0__1\n",
      "\n",
      "to\n",
      "\n",
      "results/tmp/fashionmnist_['mnist', 'notmnist', 'kmnist']/model_fsvi_cnn__architecture_four_layers__priormean_0__priorcov_0.005__optimizer_adam__lr_0.0005__bs_128__indpoints_4__indtype_ood_rand__klscale_none__nsamples_5__nmarginal_1tau_1__indlim_-1.0_1.0__reg_0__seed_0__1__complete\n",
      "\n",
      "\n",
      "------------------- DONE -------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = os.path.abspath(os.path.dirname(os.getcwd()))\n",
    "\n",
    "if os.getcwd()[-4:] == 'fsvi':\n",
    "    pass\n",
    "else:\n",
    "    os.chdir(path)\n",
    "\n",
    "%run run_base.py\\\n",
    "--data_training fashionmnist --data_ood mnist notmnist kmnist --model fsvi_cnn --architecture four_layers --activation relu --epochs 30 --learning_rate 5e-4 --optimizer adam --batch_size 128 --prior_mean 0 --prior_cov 0.005 --prior_type bnn_induced --inducing_points 4 --n_marginals 1 --kl_sup max --inducing_input_type ood_rand --inducing_input_ood_data kmnist --kl_scale none --n_samples 5 --logging_frequency 500 --seed 0 --debug --save --save_path tmp --feature_update 1 --full_cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stochastic linearization estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. (1) Linearization about value sampled from variational distribution over parameters instead of linearization about variational mean (otherwise same as baseline):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making GPU operations deterministic by setting os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_deterministic_reductions\"\"and os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
      "WARNING: UCI trainer could not be loaded\n",
      "WARNING: Offline RL trainer could not be loaded\n",
      "WARNING: Offline RL evaluator could not be loaded\n",
      "WARNING: Continual learning trainer could not be loaded\n",
      "\n",
      "Device: gpu\n",
      "\n",
      "Input arguments:\n",
      " {\n",
      "    \"data_training\":\"fashionmnist\",\n",
      "    \"data_ood\":[\n",
      "        \"mnist\",\n",
      "        \"notmnist\",\n",
      "        \"kmnist\"\n",
      "    ],\n",
      "    \"optimizer\":\"adam\",\n",
      "    \"optimizer_var\":\"not_specified\",\n",
      "    \"momentum\":0.0,\n",
      "    \"momentum_var\":0.0,\n",
      "    \"schedule\":\"not_specified\",\n",
      "    \"architecture\":\"four_layers\",\n",
      "    \"activation\":\"relu\",\n",
      "    \"prior_mean\":\"0\",\n",
      "    \"prior_cov\":\"10\",\n",
      "    \"prior_covs\":[\n",
      "        0.0\n",
      "    ],\n",
      "    \"prior_type\":\"fixed\",\n",
      "    \"epochs\":30,\n",
      "    \"start_var_opt\":0,\n",
      "    \"batch_size\":128,\n",
      "    \"learning_rate\":0.0005,\n",
      "    \"learning_rate_var\":0.001,\n",
      "    \"dropout_rate\":0.0,\n",
      "    \"regularization\":0,\n",
      "    \"n_marginals\":1,\n",
      "    \"n_condition\":128,\n",
      "    \"inducing_input_type\":\"ood_rand\",\n",
      "    \"inducing_input_ood_data\":[\n",
      "        \"kmnist\"\n",
      "    ],\n",
      "    \"inducing_input_ood_data_size\":50000,\n",
      "    \"kl_scale\":\"none\",\n",
      "    \"feature_map_jacobian\":false,\n",
      "    \"feature_map_jacobian_train_only\":false,\n",
      "    \"feature_map_type\":\"not_specified\",\n",
      "    \"td_prior_scale\":4.0,\n",
      "    \"feature_update\":1,\n",
      "    \"full_cov\":false,\n",
      "    \"n_samples\":5,\n",
      "    \"n_samples_eval\":0,\n",
      "    \"tau\":1,\n",
      "    \"noise_std\":1,\n",
      "    \"logging_frequency\":500,\n",
      "    \"figsize\":[\n",
      "        10,\n",
      "        4\n",
      "    ],\n",
      "    \"seed\":0,\n",
      "    \"save_path\":\"results/tmp/fashionmnist_['mnist', 'notmnist', 'kmnist']/model_fsvi_cnn__architecture_four_layers__priormean_0__priorcov_10__optimizer_adam__lr_0.0005__bs_128__indpoints_4__indtype_ood_rand__klscale_none__nsamples_5__nmarginal_1tau_1__indlim_-1.0_1.0__reg_0__seed_0__5\",\n",
      "    \"save\":true,\n",
      "    \"name\":\"\",\n",
      "    \"evaluate\":false,\n",
      "    \"resume_training\":false,\n",
      "    \"no_final_layer_bias\":false,\n",
      "    \"extra_linear_layer\":false,\n",
      "    \"map_initialization\":false,\n",
      "    \"stochastic_linearization\":true,\n",
      "    \"grad_flow_jacobian\":false,\n",
      "    \"stochastic_prior_mean\":\"not_specified\",\n",
      "    \"batch_normalization\":false,\n",
      "    \"batch_normalization_mod\":\"not_specified\",\n",
      "    \"final_layer_variational\":false,\n",
      "    \"kl_sup\":\"max\",\n",
      "    \"kl_sampled\":false,\n",
      "    \"fixed_inner_layers_variational_var\":false,\n",
      "    \"init_logvar\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"init_logvar_lin\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"init_logvar_conv\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"perturbation_param\":0.01,\n",
      "    \"debug\":true,\n",
      "    \"logroot\":null,\n",
      "    \"subdir\":null,\n",
      "    \"wandb_project\":\"not_specified\",\n",
      "    \"task\":\"fashionmnist_['mnist', 'notmnist', 'kmnist']\",\n",
      "    \"init_logvar_minval\":0.0,\n",
      "    \"init_logvar_maxval\":0.0,\n",
      "    \"init_logvar_lin_minval\":0.0,\n",
      "    \"init_logvar_lin_maxval\":0.0,\n",
      "    \"init_logvar_conv_minval\":0.0,\n",
      "    \"init_logvar_conv_maxval\":0.0,\n",
      "    \"model_type\":\"fsvi_cnn\",\n",
      "    \"n_inducing_inputs\":4,\n",
      "    \"inducing_inputs_bound\":[\n",
      "        -1.0,\n",
      "        1.0\n",
      "    ]\n",
      "} \n",
      "\n",
      "\n",
      "MAP initialization: False\n",
      "Full NTK computation: False\n",
      "Variational final layer: False\n",
      "Stochastic linearization (posterior): True\n",
      "Stochastic linearization (prior): False\n",
      "Gradient flow through Jacobian evaluation: False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "n_samples_eval: 10\n",
      "n_samples_train: 1\n",
      "n_batches_test: 10\n",
      "n_batches_ood: [10, 10, 10]\n",
      "n_eval: 1000\n",
      "ece_over_samples: False\n",
      "\n",
      "--- Training for 30 epochs ---\n",
      "\n",
      "----  --------  --------  --------  --------  --------  -------------------------------  ---------------------------  ---------------------------  ---------  -------------------------------  --------  --------  --------  ---------  -----------\n",
      "  ep    tr_nll    tr_acc    te_nll    te_acc    te ece  auc ent                          auc alea                     auc epis                       te_entr  ood_entr                             elbo      ellk        kl    time_ep    time_eval\n",
      "----  --------  --------  --------  --------  --------  -------------------------------  ---------------------------  ---------------------------  ---------  -------------------------------  --------  --------  --------  ---------  -----------\n",
      "   1     0.322    88.580     0.351    87.660     0.014  |   96.028   93.952   99.519  |  96.102   94.004   99.526  |  80.014   84.224   86.438  |      0.383  |    1.701    1.569    2.033  |     0.000     0.000     0.000     25.004        3.793\n",
      "   2     0.267    89.960     0.302    89.450     0.008  |   97.742   96.214   99.813  |  97.778   96.239   99.816  |  73.537   83.039   72.409  |      0.313  |    1.899    1.681    2.184  |     0.000     0.000     0.000     19.972        1.736\n",
      "   3     0.286    89.590     0.311    89.240     0.020  |   98.828   94.871   99.894  |  98.857   94.923   99.897  |  74.283   82.020   72.654  |      0.241  |    1.992    1.555    2.174  |     0.000     0.000     0.000     19.992        1.750\n",
      "   4     0.195    92.480     0.257    91.030     0.007  |   99.122   95.408   99.957  |  99.141   95.435   99.959  |  74.391   83.949   68.075  |      0.233  |    2.037    1.550    2.233  |     0.000     0.000     0.000     19.991        1.748\n",
      "   5     0.190    93.220     0.265    90.540     0.012  |   99.130   94.827   99.974  |  99.151   94.853   99.975  |  73.224   83.324   66.550  |      0.222  |    2.072    1.532    2.253  |     0.000     0.000     0.000     20.131        1.764\n",
      "   6     0.183    92.910     0.255    91.190     0.015  |   98.436   94.682   99.937  |  98.472   94.721   99.940  |  76.713   84.564   70.805  |      0.194  |    1.966    1.467    2.228  |     0.000     0.000     0.000     20.184        1.769\n",
      "   7     0.150    94.430     0.239    91.710     0.013  |   99.224   95.806   99.985  |  99.246   95.821   99.986  |  76.673   85.069   70.197  |      0.187  |    2.072    1.617    2.256  |     0.000     0.000     0.000     20.171        1.769\n",
      "   8     0.137    94.780     0.243    91.460     0.018  |   99.768   94.637   99.994  |  99.777   94.648   99.994  |  76.062   85.972   69.416  |      0.180  |    2.165    1.491    2.273  |     0.000     0.000     0.000     20.176        1.729\n",
      "   9     0.103    96.310     0.236    92.070     0.018  |   99.749   94.820   99.998  |  99.760   94.844   99.998  |  78.684   86.312   71.337  |      0.163  |    2.139    1.446    2.273  |     0.000     0.000     0.000     20.185        1.765\n",
      "  10     0.105    96.090     0.255    92.050     0.023  |   99.708   94.277   99.993  |  99.721   94.308   99.994  |  80.569   87.529   75.616  |      0.151  |    2.113    1.345    2.252  |     0.000     0.000     0.000     20.177        1.735\n",
      "  11     0.088    96.810     0.250    92.280     0.023  |   99.577   95.069   99.996  |  99.597   95.106   99.996  |  80.772   87.037   73.876  |      0.140  |    2.099    1.452    2.274  |     0.000     0.000     0.000     20.151        1.746\n",
      "  12     0.088    96.670     0.268    92.270     0.030  |   99.569   94.667   99.998  |  99.598   94.723   99.998  |  82.270   87.344   77.304  |      0.123  |    2.076    1.354    2.272  |     0.000     0.000     0.000     20.155        1.734\n",
      "  13     0.059    97.910     0.261    92.530     0.029  |   99.761   94.180   99.999  |  99.777   94.238   99.999  |  83.057   87.226   78.195  |      0.122  |    2.128    1.357    2.274  |     0.000     0.000     0.000     20.158        1.750\n",
      "  14     0.056    97.930     0.280    92.120     0.033  |   99.812   95.405   99.999  |  99.828   95.457   99.999  |  81.995   87.671   77.596  |      0.119  |    2.150    1.412    2.283  |     0.000     0.000     0.000     20.192        1.777\n",
      "  15     0.040    98.620     0.283    92.340     0.035  |   99.770   94.834   99.997  |  99.786   94.887   99.998  |  84.852   88.517   81.131  |      0.107  |    2.133    1.339    2.279  |     0.000     0.000     0.000     20.145        1.735\n",
      "  16     0.037    98.790     0.321    92.370     0.038  |   99.792   95.245   99.999  |  99.811   95.320   99.999  |  85.567   88.718   82.477  |      0.097  |    2.128    1.354    2.280  |     0.000     0.000     0.000     20.071        1.758\n",
      "  17     0.050    97.980     0.332    92.030     0.040  |   99.677   95.171   99.998  |  99.710   95.252   99.998  |  85.330   88.597   82.695  |      0.100  |    2.092    1.312    2.276  |     0.000     0.000     0.000     20.065        1.726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  18     0.030    98.880     0.363    92.140     0.044  |   99.840   96.125  100.000  |  99.858   96.208  100.000  |  87.255   89.474   85.009  |      0.086  |    2.141    1.396    2.287  |     0.000     0.000     0.000     20.111        1.737\n",
      "  19     0.046    98.220     0.383    92.070     0.046  |   99.665   95.455   99.999  |  99.705   95.584   99.999  |  87.942   89.412   85.942  |      0.082  |    2.085    1.301    2.281  |     0.000     0.000     0.000     20.098        1.747\n",
      "  20     0.021    99.240     0.378    92.380     0.044  |   99.636   95.232   99.999  |  99.681   95.335   99.999  |  88.840   89.650   87.523  |      0.080  |    2.055    1.301    2.276  |     0.000     0.000     0.000     20.105        1.753\n",
      "  21     0.047    98.130     0.445    92.020     0.049  |   99.783   96.027   99.999  |  99.818   96.128  100.000  |  88.966   89.672   87.728  |      0.076  |    2.121    1.412    2.288  |     0.000     0.000     0.000     20.085        1.752\n",
      "  22     0.012    99.660     0.414    92.180     0.047  |   99.735   96.122  100.000  |  99.767   96.250  100.000  |  89.930   90.731   88.969  |      0.079  |    2.120    1.344    2.289  |     0.000     0.000     0.000     20.094        1.746\n",
      "  23     0.024    99.090     0.445    92.080     0.048  |   99.644   95.483  100.000  |  99.699   95.628  100.000  |  90.215   89.823   89.239  |      0.075  |    2.076    1.330    2.287  |     0.000     0.000     0.000     19.998        1.742\n",
      "  24     0.017    99.310     0.474    92.070     0.051  |   99.860   96.691   99.999  |  99.886   96.832   99.999  |  91.354   91.180   90.686  |      0.069  |    2.141    1.399    2.289  |     0.000     0.000     0.000     20.014        1.760\n",
      "  25     0.013    99.570     0.454    92.230     0.049  |   99.773   95.993   99.999  |  99.810   96.092   99.999  |  92.054   90.989   91.493  |      0.071  |    2.092    1.404    2.287  |     0.000     0.000     0.000     20.167        1.728\n",
      "  26     0.012    99.660     0.468    92.330     0.048  |   99.853   96.132   99.999  |  99.875   96.246  100.000  |  93.058   91.666   92.490  |      0.069  |    2.137    1.371    2.287  |     0.000     0.000     0.000     20.110        1.732\n",
      "  27     0.019    99.360     0.473    92.100     0.051  |   99.826   96.799   99.999  |  99.852   96.895   99.999  |  93.699   92.433   93.398  |      0.070  |    2.135    1.473    2.290  |     0.000     0.000     0.000     20.195        1.757\n",
      "  28     0.009    99.680     0.498    92.500     0.048  |   99.776   96.847   99.999  |  99.814   96.973   99.999  |  94.205   92.962   94.025  |      0.066  |    2.091    1.422    2.286  |     0.000     0.000     0.000     20.194        1.746\n",
      "  29     0.009    99.680     0.510    92.230     0.051  |   99.696   95.868  100.000  |  99.736   95.988  100.000  |  95.535   92.751   95.428  |      0.065  |    2.041    1.289    2.274  |     0.000     0.000     0.000     20.153        1.777\n",
      "  30     0.015    99.530     0.561    92.490     0.051  |   99.845   96.010   99.999  |  99.869   96.151   99.999  |  96.685   92.521   96.778  |      0.061  |    2.120    1.307    2.280  |     0.000     0.000     0.000     20.116        1.757\n",
      "\n",
      "Changing save path from\n",
      "\n",
      "results/tmp/fashionmnist_['mnist', 'notmnist', 'kmnist']/model_fsvi_cnn__architecture_four_layers__priormean_0__priorcov_10__optimizer_adam__lr_0.0005__bs_128__indpoints_4__indtype_ood_rand__klscale_none__nsamples_5__nmarginal_1tau_1__indlim_-1.0_1.0__reg_0__seed_0__5\n",
      "\n",
      "to\n",
      "\n",
      "results/tmp/fashionmnist_['mnist', 'notmnist', 'kmnist']/model_fsvi_cnn__architecture_four_layers__priormean_0__priorcov_10__optimizer_adam__lr_0.0005__bs_128__indpoints_4__indtype_ood_rand__klscale_none__nsamples_5__nmarginal_1tau_1__indlim_-1.0_1.0__reg_0__seed_0__5__complete\n",
      "\n",
      "\n",
      "------------------- DONE -------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = os.path.abspath(os.path.dirname(os.getcwd()))\n",
    "\n",
    "if os.getcwd()[-4:] == 'fsvi':\n",
    "    pass\n",
    "else:\n",
    "    os.chdir(path)\n",
    "\n",
    "%run run_base.py\\\n",
    "--data_training fashionmnist --data_ood mnist notmnist kmnist --model fsvi_cnn --architecture four_layers --activation relu --epochs 30 --learning_rate 5e-4 --optimizer adam --batch_size 128 --prior_mean 0 --prior_cov 10 --prior_type fixed --inducing_points 4 --n_marginals 1 --kl_sup max --inducing_input_type ood_rand --inducing_input_ood_data kmnist --kl_scale none --n_samples 5 --logging_frequency 500 --seed 0 --debug --save --save_path tmp --feature_update 1 --stochastic_linearization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. (1) Linearization about value sampled from variational distribution over parameters instead of linearization about variational mean + (2) gradient flow through linearization evaluation points (otherwise same as baseline):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making GPU operations deterministic by setting os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_deterministic_reductions\"\"and os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
      "WARNING: UCI trainer could not be loaded\n",
      "WARNING: Offline RL trainer could not be loaded\n",
      "WARNING: Offline RL evaluator could not be loaded\n",
      "WARNING: Continual learning trainer could not be loaded\n",
      "\n",
      "Device: gpu\n",
      "\n",
      "Input arguments:\n",
      " {\n",
      "    \"data_training\":\"fashionmnist\",\n",
      "    \"data_ood\":[\n",
      "        \"mnist\",\n",
      "        \"notmnist\",\n",
      "        \"kmnist\"\n",
      "    ],\n",
      "    \"optimizer\":\"adam\",\n",
      "    \"optimizer_var\":\"not_specified\",\n",
      "    \"momentum\":0.0,\n",
      "    \"momentum_var\":0.0,\n",
      "    \"schedule\":\"not_specified\",\n",
      "    \"architecture\":\"four_layers\",\n",
      "    \"activation\":\"relu\",\n",
      "    \"prior_mean\":\"0\",\n",
      "    \"prior_cov\":\"10\",\n",
      "    \"prior_covs\":[\n",
      "        0.0\n",
      "    ],\n",
      "    \"prior_type\":\"fixed\",\n",
      "    \"epochs\":30,\n",
      "    \"start_var_opt\":0,\n",
      "    \"batch_size\":128,\n",
      "    \"learning_rate\":0.0005,\n",
      "    \"learning_rate_var\":0.001,\n",
      "    \"dropout_rate\":0.0,\n",
      "    \"regularization\":0,\n",
      "    \"n_marginals\":1,\n",
      "    \"n_condition\":128,\n",
      "    \"inducing_input_type\":\"ood_rand\",\n",
      "    \"inducing_input_ood_data\":[\n",
      "        \"kmnist\"\n",
      "    ],\n",
      "    \"inducing_input_ood_data_size\":50000,\n",
      "    \"kl_scale\":\"none\",\n",
      "    \"feature_map_jacobian\":false,\n",
      "    \"feature_map_jacobian_train_only\":false,\n",
      "    \"feature_map_type\":\"not_specified\",\n",
      "    \"td_prior_scale\":4.0,\n",
      "    \"feature_update\":1,\n",
      "    \"full_cov\":false,\n",
      "    \"n_samples\":5,\n",
      "    \"n_samples_eval\":0,\n",
      "    \"tau\":1,\n",
      "    \"noise_std\":1,\n",
      "    \"logging_frequency\":500,\n",
      "    \"figsize\":[\n",
      "        10,\n",
      "        4\n",
      "    ],\n",
      "    \"seed\":0,\n",
      "    \"save_path\":\"results/tmp/fashionmnist_['mnist', 'notmnist', 'kmnist']/model_fsvi_cnn__architecture_four_layers__priormean_0__priorcov_10__optimizer_adam__lr_0.0005__bs_128__indpoints_4__indtype_ood_rand__klscale_none__nsamples_5__nmarginal_1tau_1__indlim_-1.0_1.0__reg_0__seed_0__6\",\n",
      "    \"save\":true,\n",
      "    \"name\":\"\",\n",
      "    \"evaluate\":false,\n",
      "    \"resume_training\":false,\n",
      "    \"no_final_layer_bias\":false,\n",
      "    \"extra_linear_layer\":false,\n",
      "    \"map_initialization\":false,\n",
      "    \"stochastic_linearization\":true,\n",
      "    \"grad_flow_jacobian\":true,\n",
      "    \"stochastic_prior_mean\":\"not_specified\",\n",
      "    \"batch_normalization\":false,\n",
      "    \"batch_normalization_mod\":\"not_specified\",\n",
      "    \"final_layer_variational\":false,\n",
      "    \"kl_sup\":\"max\",\n",
      "    \"kl_sampled\":false,\n",
      "    \"fixed_inner_layers_variational_var\":false,\n",
      "    \"init_logvar\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"init_logvar_lin\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"init_logvar_conv\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"perturbation_param\":0.01,\n",
      "    \"debug\":true,\n",
      "    \"logroot\":null,\n",
      "    \"subdir\":null,\n",
      "    \"wandb_project\":\"not_specified\",\n",
      "    \"task\":\"fashionmnist_['mnist', 'notmnist', 'kmnist']\",\n",
      "    \"init_logvar_minval\":0.0,\n",
      "    \"init_logvar_maxval\":0.0,\n",
      "    \"init_logvar_lin_minval\":0.0,\n",
      "    \"init_logvar_lin_maxval\":0.0,\n",
      "    \"init_logvar_conv_minval\":0.0,\n",
      "    \"init_logvar_conv_maxval\":0.0,\n",
      "    \"model_type\":\"fsvi_cnn\",\n",
      "    \"n_inducing_inputs\":4,\n",
      "    \"inducing_inputs_bound\":[\n",
      "        -1.0,\n",
      "        1.0\n",
      "    ]\n",
      "} \n",
      "\n",
      "\n",
      "MAP initialization: False\n",
      "Full NTK computation: False\n",
      "Variational final layer: False\n",
      "Stochastic linearization (posterior): True\n",
      "Stochastic linearization (prior): False\n",
      "Gradient flow through Jacobian evaluation: True\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "n_samples_eval: 10\n",
      "n_samples_train: 1\n",
      "n_batches_test: 10\n",
      "n_batches_ood: [10, 10, 10]\n",
      "n_eval: 1000\n",
      "ece_over_samples: False\n",
      "\n",
      "--- Training for 30 epochs ---\n",
      "\n",
      "----  --------  --------  --------  --------  --------  -------------------------------  ---------------------------  ---------------------------  ---------  -------------------------------  --------  --------  --------  ---------  -----------\n",
      "  ep    tr_nll    tr_acc    te_nll    te_acc    te ece  auc ent                          auc alea                     auc epis                       te_entr  ood_entr                             elbo      ellk        kl    time_ep    time_eval\n",
      "----  --------  --------  --------  --------  --------  -------------------------------  ---------------------------  ---------------------------  ---------  -------------------------------  --------  --------  --------  ---------  -----------\n",
      "   1     0.477    83.400     0.439    85.400     0.014  |   82.942   84.921   97.306  |  70.438   78.158   87.018  |  94.003   92.345   99.581  |      0.434  |    1.167    1.159    1.687  |     0.000     0.000     0.000     56.802        3.678\n",
      "   2     0.345    87.420     0.352    88.040     0.013  |   96.648   91.603   99.761  |  84.338   84.773   91.480  |  99.577   96.366   99.978  |      0.359  |    1.706    1.332    1.976  |     0.000     0.000     0.000     50.691        1.739\n",
      "   3     0.331    88.530     0.333    88.460     0.007  |   98.897   92.728   99.881  |  85.406   86.363   87.233  |  99.942   96.651   99.996  |      0.304  |    1.824    1.370    1.928  |     0.000     0.000     0.000     50.510        1.755\n",
      "   4     0.233    91.370     0.280    90.240     0.006  |   99.696   91.315   99.913  |  86.901   84.961   88.628  |  99.990   95.952   99.995  |      0.275  |    1.876    1.265    1.935  |     0.000     0.000     0.000     50.702        1.745\n",
      "   5     0.204    92.460     0.269    90.620     0.005  |   99.189   94.890   99.948  |  90.290   89.867   90.844  |  99.936   97.449   99.997  |      0.246  |    1.881    1.446    1.981  |     0.000     0.000     0.000     50.725        1.788\n",
      "   6     0.186    92.910     0.254    90.910     0.005  |   99.259   95.346   99.940  |  91.383   90.726   92.651  |  99.932   97.648   99.995  |      0.234  |    1.897    1.448    2.007  |     0.000     0.000     0.000     50.715        1.728\n",
      "   7     0.175    93.590     0.247    91.260     0.008  |   99.841   96.914   99.953  |  89.045   91.940   89.584  |  99.991   98.327   99.998  |      0.219  |    1.932    1.573    1.959  |     0.000     0.000     0.000     50.603        1.729\n",
      "   8     0.165    93.810     0.244    91.240     0.009  |   99.886   95.666   99.982  |  93.085   91.837   93.560  |  99.989   97.546   99.999  |      0.211  |    1.998    1.514    2.026  |     0.000     0.000     0.000     50.633        1.733\n",
      "   9     0.134    95.310     0.238    91.780     0.011  |   99.827   96.370   99.981  |  94.257   93.224   94.793  |  99.982   98.001   99.999  |      0.192  |    1.973    1.544    2.028  |     0.000     0.000     0.000     50.605        1.746\n",
      "  10     0.137    94.960     0.240    92.070     0.012  |   99.705   96.003   99.992  |  94.823   93.112   95.995  |  99.942   97.787   99.998  |      0.188  |    2.002    1.538    2.100  |     0.000     0.000     0.000     50.530        1.737\n",
      "  11     0.119    95.570     0.227    92.480     0.012  |   99.837   96.269   99.987  |  95.181   93.361   94.874  |  99.969   97.547   99.999  |      0.175  |    1.958    1.544    1.963  |     0.000     0.000     0.000     50.581        1.745\n",
      "  12     0.112    95.910     0.234    92.320     0.016  |   99.927   95.637   99.995  |  95.386   93.308   95.374  |  99.988   97.078   99.998  |      0.163  |    1.992    1.505    2.006  |     0.000     0.000     0.000     50.775        1.740\n",
      "  13     0.097    96.500     0.237    92.420     0.018  |   99.879   95.498   99.998  |  96.222   93.299   96.682  |  99.969   97.094   99.999  |      0.153  |    1.977    1.460    2.020  |     0.000     0.000     0.000     50.726        1.744\n",
      "  14     0.086    96.740     0.247    92.430     0.024  |   99.935   95.769   99.987  |  96.626   93.845   96.998  |  99.989   97.230   99.999  |      0.143  |    1.975    1.480    2.018  |     0.000     0.000     0.000     50.674        1.741\n",
      "  15     0.079    97.300     0.247    92.400     0.025  |   99.821   95.758   99.981  |  96.161   93.372   96.188  |  99.946   97.125   99.998  |      0.139  |    1.933    1.476    1.973  |     0.000     0.000     0.000     50.669        1.766\n",
      "  16     0.060    97.750     0.271    92.400     0.030  |   99.840   95.790   99.993  |  97.218   93.946   97.236  |  99.940   97.001   99.999  |      0.120  |    1.932    1.425    1.979  |     0.000     0.000     0.000     50.577        1.753\n",
      "  17     0.055    98.140     0.266    92.160     0.029  |   99.885   97.014  100.000  |  97.853   95.629   97.590  |  99.952   97.846   99.999  |      0.127  |    1.969    1.552    1.996  |     0.000     0.000     0.000     50.592        1.752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  18     0.040    98.870     0.279    92.220     0.034  |   99.840   96.956   99.995  |  98.005   95.867   98.080  |  99.933   97.645   99.998  |      0.112  |    1.946    1.538    1.985  |     0.000     0.000     0.000     50.628        1.737\n",
      "  19     0.039    98.630     0.299    92.460     0.035  |   99.755   96.208   99.990  |  98.003   95.045   98.405  |  99.879   97.090   99.999  |      0.104  |    1.915    1.449    1.986  |     0.000     0.000     0.000     50.662        1.761\n",
      "  20     0.037    98.760     0.293    92.310     0.035  |   99.747   96.636   99.986  |  98.559   95.765   98.731  |  99.869   97.361   99.998  |      0.108  |    1.966    1.526    2.011  |     0.000     0.000     0.000     50.608        1.758\n",
      "  21     0.043    98.310     0.340    91.890     0.043  |   99.807   96.504   99.990  |  98.491   95.512   98.581  |  99.895   97.178   99.999  |      0.097  |    1.929    1.477    1.975  |     0.000     0.000     0.000     50.601        1.734\n",
      "  22     0.023    99.270     0.328    92.370     0.042  |   99.764   96.381   99.996  |  98.461   95.450   98.637  |  99.856   97.029   99.999  |      0.087  |    1.914    1.430    1.970  |     0.000     0.000     0.000     50.553        1.745\n",
      "  23     0.020    99.320     0.344    92.250     0.043  |   99.876   96.924   99.975  |  98.433   96.059   98.549  |  99.948   97.437  100.000  |      0.087  |    1.935    1.485    1.977  |     0.000     0.000     0.000     50.573        1.732\n",
      "  24     0.016    99.490     0.366    92.340     0.046  |   99.882   96.648   99.982  |  98.885   95.850   98.977  |  99.941   97.186  100.000  |      0.079  |    1.927    1.446    1.980  |     0.000     0.000     0.000     50.585        1.752\n",
      "  25     0.011    99.710     0.380    92.460     0.045  |   99.793   96.669   99.999  |  98.793   95.905   98.953  |  99.872   97.093  100.000  |      0.075  |    1.893    1.434    1.963  |     0.000     0.000     0.000     50.591        1.747\n",
      "  26     0.019    99.410     0.383    92.380     0.045  |   99.849   97.217   99.997  |  99.108   96.558   99.212  |  99.899   97.551   99.999  |      0.076  |    1.927    1.494    1.976  |     0.000     0.000     0.000     50.618        1.754\n",
      "  27     0.022    99.190     0.413    92.080     0.049  |   99.862   96.904   99.990  |  99.062   96.175   99.125  |  99.921   97.339  100.000  |      0.076  |    1.949    1.473    1.993  |     0.000     0.000     0.000     50.518        1.733\n",
      "  28     0.008    99.790     0.401    92.410     0.048  |   99.788   97.022   99.999  |  99.130   96.432   99.377  |  99.858   97.321  100.000  |      0.069  |    1.935    1.483    2.007  |     0.000     0.000     0.000     50.485        1.756\n",
      "  29     0.008    99.780     0.429    92.350     0.050  |   99.911   96.810   99.984  |  99.172   96.243   99.345  |  99.962   97.133  100.000  |      0.067  |    1.934    1.456    1.993  |     0.000     0.000     0.000     50.571        1.740\n",
      "  30     0.009    99.790     0.471    91.990     0.053  |   99.766   97.063   99.996  |  99.041   96.474   99.199  |  99.848   97.313  100.000  |      0.066  |    1.908    1.464    1.980  |     0.000     0.000     0.000     50.616        1.755\n",
      "\n",
      "Changing save path from\n",
      "\n",
      "results/tmp/fashionmnist_['mnist', 'notmnist', 'kmnist']/model_fsvi_cnn__architecture_four_layers__priormean_0__priorcov_10__optimizer_adam__lr_0.0005__bs_128__indpoints_4__indtype_ood_rand__klscale_none__nsamples_5__nmarginal_1tau_1__indlim_-1.0_1.0__reg_0__seed_0__6\n",
      "\n",
      "to\n",
      "\n",
      "results/tmp/fashionmnist_['mnist', 'notmnist', 'kmnist']/model_fsvi_cnn__architecture_four_layers__priormean_0__priorcov_10__optimizer_adam__lr_0.0005__bs_128__indpoints_4__indtype_ood_rand__klscale_none__nsamples_5__nmarginal_1tau_1__indlim_-1.0_1.0__reg_0__seed_0__6__complete\n",
      "\n",
      "\n",
      "------------------- DONE -------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = os.path.abspath(os.path.dirname(os.getcwd()))\n",
    "\n",
    "if os.getcwd()[-4:] == 'fsvi':\n",
    "    pass\n",
    "else:\n",
    "    os.chdir(path)\n",
    "\n",
    "%run run_base.py\\\n",
    "--data_training fashionmnist --data_ood mnist notmnist kmnist --model fsvi_cnn --architecture four_layers --activation relu --epochs 30 --learning_rate 5e-4 --optimizer adam --batch_size 128 --prior_mean 0 --prior_cov 10 --prior_type fixed --inducing_points 4 --n_marginals 1 --kl_sup max --inducing_input_type ood_rand --inducing_input_ood_data kmnist --kl_scale none --n_samples 5 --logging_frequency 500 --seed 0 --debug --save --save_path tmp --feature_update 1 --stochastic_linearization --grad_flow_jacobian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Learn only final-layer variational distribution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. (1) Randomness in non-final layers generated by dropout (otherwise same as baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making GPU operations deterministic by setting os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_deterministic_reductions\"\"and os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
      "WARNING: UCI trainer could not be loaded\n",
      "WARNING: Offline RL trainer could not be loaded\n",
      "WARNING: Offline RL evaluator could not be loaded\n",
      "WARNING: Continual learning trainer could not be loaded\n",
      "\n",
      "Device: gpu\n",
      "\n",
      "Input arguments:\n",
      " {\n",
      "    \"data_training\":\"fashionmnist\",\n",
      "    \"data_ood\":[\n",
      "        \"mnist\",\n",
      "        \"notmnist\",\n",
      "        \"kmnist\"\n",
      "    ],\n",
      "    \"optimizer\":\"adam\",\n",
      "    \"optimizer_var\":\"not_specified\",\n",
      "    \"momentum\":0.0,\n",
      "    \"momentum_var\":0.0,\n",
      "    \"schedule\":\"not_specified\",\n",
      "    \"architecture\":\"four_layers\",\n",
      "    \"activation\":\"relu\",\n",
      "    \"prior_mean\":\"0\",\n",
      "    \"prior_cov\":\"10\",\n",
      "    \"prior_covs\":[\n",
      "        0.0\n",
      "    ],\n",
      "    \"prior_type\":\"fixed\",\n",
      "    \"epochs\":30,\n",
      "    \"start_var_opt\":0,\n",
      "    \"batch_size\":128,\n",
      "    \"learning_rate\":0.0005,\n",
      "    \"learning_rate_var\":0.001,\n",
      "    \"dropout_rate\":0.1,\n",
      "    \"regularization\":0,\n",
      "    \"n_marginals\":1,\n",
      "    \"n_condition\":128,\n",
      "    \"inducing_input_type\":\"ood_rand\",\n",
      "    \"inducing_input_ood_data\":[\n",
      "        \"kmnist\"\n",
      "    ],\n",
      "    \"inducing_input_ood_data_size\":50000,\n",
      "    \"kl_scale\":\"none\",\n",
      "    \"feature_map_jacobian\":false,\n",
      "    \"feature_map_jacobian_train_only\":false,\n",
      "    \"feature_map_type\":\"not_specified\",\n",
      "    \"td_prior_scale\":4.0,\n",
      "    \"feature_update\":1,\n",
      "    \"full_cov\":false,\n",
      "    \"n_samples\":5,\n",
      "    \"n_samples_eval\":0,\n",
      "    \"tau\":1,\n",
      "    \"noise_std\":1,\n",
      "    \"logging_frequency\":500,\n",
      "    \"figsize\":[\n",
      "        10,\n",
      "        4\n",
      "    ],\n",
      "    \"seed\":0,\n",
      "    \"save_path\":\"results/tmp/fashionmnist_['mnist', 'notmnist', 'kmnist']/model_fsvi_cnn_dropout__architecture_four_layers__priormean_0__priorcov_10__optimizer_adam__lr_0.0005__bs_128__indpoints_4__indtype_ood_rand__klscale_none__nsamples_5__nmarginal_1tau_1__indlim_-1.0_1.0__reg_0__seed_0__3\",\n",
      "    \"save\":true,\n",
      "    \"name\":\"\",\n",
      "    \"evaluate\":false,\n",
      "    \"resume_training\":false,\n",
      "    \"no_final_layer_bias\":false,\n",
      "    \"extra_linear_layer\":false,\n",
      "    \"map_initialization\":false,\n",
      "    \"stochastic_linearization\":false,\n",
      "    \"grad_flow_jacobian\":false,\n",
      "    \"stochastic_prior_mean\":\"not_specified\",\n",
      "    \"batch_normalization\":false,\n",
      "    \"batch_normalization_mod\":\"not_specified\",\n",
      "    \"final_layer_variational\":true,\n",
      "    \"kl_sup\":\"max\",\n",
      "    \"kl_sampled\":false,\n",
      "    \"fixed_inner_layers_variational_var\":false,\n",
      "    \"init_logvar\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"init_logvar_lin\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"init_logvar_conv\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"perturbation_param\":0.01,\n",
      "    \"debug\":true,\n",
      "    \"logroot\":null,\n",
      "    \"subdir\":null,\n",
      "    \"wandb_project\":\"not_specified\",\n",
      "    \"task\":\"fashionmnist_['mnist', 'notmnist', 'kmnist']\",\n",
      "    \"init_logvar_minval\":0.0,\n",
      "    \"init_logvar_maxval\":0.0,\n",
      "    \"init_logvar_lin_minval\":0.0,\n",
      "    \"init_logvar_lin_maxval\":0.0,\n",
      "    \"init_logvar_conv_minval\":0.0,\n",
      "    \"init_logvar_conv_maxval\":0.0,\n",
      "    \"model_type\":\"fsvi_cnn_dropout\",\n",
      "    \"n_inducing_inputs\":4,\n",
      "    \"inducing_inputs_bound\":[\n",
      "        -1.0,\n",
      "        1.0\n",
      "    ]\n",
      "} \n",
      "\n",
      "\n",
      "MAP initialization: False\n",
      "Full NTK computation: False\n",
      "Variational final layer: True\n",
      "Stochastic linearization (posterior): False\n",
      "Stochastic linearization (prior): False\n",
      "Gradient flow through Jacobian evaluation: False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "n_samples_eval: 10\n",
      "n_samples_train: 1\n",
      "n_batches_test: 10\n",
      "n_batches_ood: [10, 10, 10]\n",
      "n_eval: 1000\n",
      "ece_over_samples: False\n",
      "\n",
      "--- Training for 30 epochs ---\n",
      "\n",
      "----  --------  --------  --------  --------  --------  -------------------------------  ---------------------------  ---------------------------  ---------  -------------------------------  --------  --------  --------  ---------  -----------\n",
      "  ep    tr_nll    tr_acc    te_nll    te_acc    te ece  auc ent                          auc alea                     auc epis                       te_entr  ood_entr                             elbo      ellk        kl    time_ep    time_eval\n",
      "----  --------  --------  --------  --------  --------  -------------------------------  ---------------------------  ---------------------------  ---------  -------------------------------  --------  --------  --------  ---------  -----------\n",
      "   1     0.314    88.680     0.351    87.880     0.020  |   97.971   95.496   99.770  |  98.029   95.552   99.775  |  65.545   66.791   58.158  |      0.399  |    1.942    1.784    2.176  |     0.000     0.000     0.000     14.612        4.442\n",
      "   2     0.263    90.240     0.304    89.330     0.016  |   97.472   96.061   99.856  |  97.526   96.101   99.859  |  65.181   72.161   49.855  |      0.335  |    1.930    1.773    2.229  |     0.000     0.000     0.000     10.861        3.137\n",
      "   3     0.275    89.940     0.316    89.110     0.012  |   98.718   95.235   99.939  |  98.762   95.287   99.942  |  70.830   75.992   60.648  |      0.265  |    1.987    1.652    2.210  |     0.000     0.000     0.000     10.813        3.128\n",
      "   4     0.210    92.060     0.268    90.690     0.006  |   98.435   96.248   99.962  |  98.468   96.257   99.963  |  69.299   78.954   51.660  |      0.270  |    1.983    1.706    2.253  |     0.000     0.000     0.000     10.818        3.100\n",
      "   5     0.194    92.870     0.263    90.700     0.006  |   98.221   94.488   99.976  |  98.266   94.459   99.978  |  74.671   82.810   60.194  |      0.232  |    1.928    1.538    2.247  |     0.000     0.000     0.000     10.845        3.132\n",
      "   6     0.177    93.240     0.249    91.510     0.004  |   97.056   95.207   99.951  |  97.125   95.205   99.955  |  74.126   80.540   58.101  |      0.218  |    1.853    1.623    2.252  |     0.000     0.000     0.000     11.011        3.109\n",
      "   7     0.148    94.840     0.234    91.980     0.005  |   99.032   96.256   99.994  |  99.061   96.247   99.995  |  74.433   81.547   58.358  |      0.206  |    2.030    1.688    2.270  |     0.000     0.000     0.000     10.925        3.091\n",
      "   8     0.138    94.690     0.240    91.820     0.003  |   99.610   96.078   99.999  |  99.626   96.044   99.999  |  71.193   81.488   55.246  |      0.214  |    2.127    1.681    2.283  |     0.000     0.000     0.000     10.826        3.123\n",
      "   9     0.120    95.770     0.233    92.180     0.006  |   99.713   95.565   99.999  |  99.726   95.484   99.999  |  73.576   82.219   56.526  |      0.195  |    2.123    1.652    2.290  |     0.000     0.000     0.000     11.046        3.115\n",
      "  10     0.108    96.010     0.233    92.730     0.008  |   99.572   95.805   99.999  |  99.596   95.768   99.999  |  75.886   83.384   60.952  |      0.188  |    2.090    1.632    2.282  |     0.000     0.000     0.000     10.841        3.124\n",
      "  11     0.100    96.370     0.231    92.780     0.007  |   99.206   95.289   99.997  |  99.243   95.253   99.997  |  78.495   84.925   63.503  |      0.174  |    2.024    1.551    2.281  |     0.000     0.000     0.000     10.820        3.122\n",
      "  12     0.089    96.660     0.251    92.760     0.017  |   99.306   95.579   99.999  |  99.368   95.571   99.999  |  81.069   85.950   68.924  |      0.148  |    1.989    1.512    2.280  |     0.000     0.000     0.000     10.852        3.121\n",
      "  13     0.080    97.250     0.246    92.690     0.013  |   99.767   95.329   99.999  |  99.795   95.284   99.999  |  78.321   84.772   66.271  |      0.161  |    2.104    1.549    2.286  |     0.000     0.000     0.000     10.846        3.138\n",
      "  14     0.078    97.030     0.262    92.520     0.015  |   99.802   96.910  100.000  |  99.823   96.869  100.000  |  77.902   83.494   65.539  |      0.159  |    2.136    1.744    2.294  |     0.000     0.000     0.000     10.856        3.130\n",
      "  15     0.057    97.980     0.267    92.620     0.019  |   99.807   95.554  100.000  |  99.833   95.521  100.000  |  81.131   84.557   70.324  |      0.144  |    2.076    1.591    2.289  |     0.000     0.000     0.000     10.839        3.139\n",
      "  16     0.050    98.300     0.273    92.760     0.020  |   99.803   95.412  100.000  |  99.834   95.402  100.000  |  80.279   85.549   72.177  |      0.137  |    2.123    1.527    2.290  |     0.000     0.000     0.000     10.848        3.134\n",
      "  17     0.048    98.340     0.287    92.780     0.020  |   99.792   95.532   99.999  |  99.827   95.504   99.999  |  81.137   86.017   73.287  |      0.136  |    2.109    1.505    2.287  |     0.000     0.000     0.000     10.839        3.139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  18     0.040    98.630     0.287    92.880     0.021  |   99.824   95.997  100.000  |  99.853   95.980  100.000  |  80.965   85.824   74.767  |      0.130  |    2.145    1.589    2.294  |     0.000     0.000     0.000     10.942        3.143\n",
      "  19     0.036    98.740     0.311    92.460     0.028  |   99.781   96.260  100.000  |  99.824   96.296  100.000  |  83.279   86.812   77.118  |      0.120  |    2.101    1.544    2.291  |     0.000     0.000     0.000     11.051        3.144\n",
      "  20     0.029    99.070     0.301    93.000     0.024  |   99.741   95.347  100.000  |  99.796   95.393  100.000  |  82.693   85.986   77.913  |      0.119  |    2.099    1.516    2.292  |     0.000     0.000     0.000     11.068        3.130\n",
      "  21     0.038    98.620     0.354    92.330     0.032  |   99.770   96.439  100.000  |  99.831   96.517  100.000  |  82.407   85.959   78.724  |      0.113  |    2.137    1.595    2.294  |     0.000     0.000     0.000     10.835        3.109\n",
      "  22     0.026    99.150     0.335    93.080     0.027  |   99.930   95.851  100.000  |  99.946   95.886  100.000  |  83.921   87.746   80.955  |      0.108  |    2.185    1.484    2.293  |     0.000     0.000     0.000     10.819        3.087\n",
      "  23     0.026    99.170     0.345    92.840     0.028  |   99.930   96.664  100.000  |  99.946   96.706  100.000  |  84.434   87.937   81.950  |      0.111  |    2.192    1.593    2.293  |     0.000     0.000     0.000     10.839        3.135\n",
      "  24     0.016    99.410     0.369    92.880     0.031  |   99.915   95.219  100.000  |  99.939   95.361  100.000  |  86.019   87.812   83.652  |      0.099  |    2.168    1.415    2.291  |     0.000     0.000     0.000     10.887        3.132\n",
      "  25     0.018    99.390     0.367    93.040     0.028  |   99.849   95.998  100.000  |  99.891   96.084  100.000  |  86.434   88.280   83.823  |      0.106  |    2.124    1.507    2.287  |     0.000     0.000     0.000     11.047        3.144\n",
      "  26     0.016    99.510     0.373    93.070     0.027  |   99.936   96.258  100.000  |  99.956   96.327  100.000  |  86.305   88.652   84.503  |      0.106  |    2.196    1.517    2.290  |     0.000     0.000     0.000     11.032        3.110\n",
      "  27     0.020    99.300     0.390    92.750     0.031  |   99.890   95.711   99.999  |  99.921   95.790  100.000  |  87.724   88.895   85.825  |      0.105  |    2.167    1.495    2.288  |     0.000     0.000     0.000     11.028        3.092\n",
      "  28     0.013    99.550     0.398    93.130     0.029  |   99.951   96.260  100.000  |  99.967   96.363  100.000  |  88.536   89.370   87.080  |      0.100  |    2.179    1.558    2.286  |     0.000     0.000     0.000     11.015        3.117\n",
      "  29     0.009    99.800     0.417    93.140     0.029  |   99.938   95.357  100.000  |  99.959   95.495  100.000  |  89.759   89.093   88.097  |      0.097  |    2.146    1.464    2.283  |     0.000     0.000     0.000     11.029        3.128\n",
      "  30     0.011    99.590     0.437    93.080     0.031  |   99.652   96.257  100.000  |  99.749   96.386  100.000  |  91.011   90.940   89.392  |      0.095  |    2.054    1.480    2.278  |     0.000     0.000     0.000     11.045        3.115\n",
      "\n",
      "Changing save path from\n",
      "\n",
      "results/tmp/fashionmnist_['mnist', 'notmnist', 'kmnist']/model_fsvi_cnn_dropout__architecture_four_layers__priormean_0__priorcov_10__optimizer_adam__lr_0.0005__bs_128__indpoints_4__indtype_ood_rand__klscale_none__nsamples_5__nmarginal_1tau_1__indlim_-1.0_1.0__reg_0__seed_0__3\n",
      "\n",
      "to\n",
      "\n",
      "results/tmp/fashionmnist_['mnist', 'notmnist', 'kmnist']/model_fsvi_cnn_dropout__architecture_four_layers__priormean_0__priorcov_10__optimizer_adam__lr_0.0005__bs_128__indpoints_4__indtype_ood_rand__klscale_none__nsamples_5__nmarginal_1tau_1__indlim_-1.0_1.0__reg_0__seed_0__3__complete\n",
      "\n",
      "\n",
      "------------------- DONE -------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = os.path.abspath(os.path.dirname(os.getcwd()))\n",
    "\n",
    "if os.getcwd()[-4:] == 'fsvi':\n",
    "    pass\n",
    "else:\n",
    "    os.chdir(path)\n",
    "\n",
    "%run run_base.py\\\n",
    "--data_training fashionmnist --data_ood mnist notmnist kmnist --model fsvi_cnn_dropout --architecture four_layers --activation relu --epochs 30 --learning_rate 5e-4 --optimizer adam --batch_size 128 --prior_mean 0 --prior_cov 10 --prior_type fixed --inducing_points 4 --n_marginals 1 --kl_sup max --inducing_input_type ood_rand --inducing_input_ood_data kmnist --kl_scale none --n_samples 5 --logging_frequency 500 --seed 0 --debug --save --save_path tmp --feature_update 1 --final_layer_variational --dropout_rate 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. (1) Randomness in non-final layers generated by dropout + (2) linearization about value sampled from variational distribution over parameters instead about variational mean (otherwise same as baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making GPU operations deterministic by setting os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_deterministic_reductions\"\"and os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
      "WARNING: UCI trainer could not be loaded\n",
      "WARNING: Offline RL trainer could not be loaded\n",
      "WARNING: Offline RL evaluator could not be loaded\n",
      "WARNING: Continual learning trainer could not be loaded\n",
      "\n",
      "Device: gpu\n",
      "\n",
      "Input arguments:\n",
      " {\n",
      "    \"data_training\":\"fashionmnist\",\n",
      "    \"data_ood\":[\n",
      "        \"mnist\",\n",
      "        \"notmnist\",\n",
      "        \"kmnist\"\n",
      "    ],\n",
      "    \"optimizer\":\"adam\",\n",
      "    \"optimizer_var\":\"not_specified\",\n",
      "    \"momentum\":0.0,\n",
      "    \"momentum_var\":0.0,\n",
      "    \"schedule\":\"not_specified\",\n",
      "    \"architecture\":\"four_layers\",\n",
      "    \"activation\":\"relu\",\n",
      "    \"prior_mean\":\"0\",\n",
      "    \"prior_cov\":\"10\",\n",
      "    \"prior_covs\":[\n",
      "        0.0\n",
      "    ],\n",
      "    \"prior_type\":\"fixed\",\n",
      "    \"epochs\":30,\n",
      "    \"start_var_opt\":0,\n",
      "    \"batch_size\":128,\n",
      "    \"learning_rate\":0.0005,\n",
      "    \"learning_rate_var\":0.001,\n",
      "    \"dropout_rate\":0.1,\n",
      "    \"regularization\":0,\n",
      "    \"n_marginals\":1,\n",
      "    \"n_condition\":128,\n",
      "    \"inducing_input_type\":\"ood_rand\",\n",
      "    \"inducing_input_ood_data\":[\n",
      "        \"kmnist\"\n",
      "    ],\n",
      "    \"inducing_input_ood_data_size\":50000,\n",
      "    \"kl_scale\":\"none\",\n",
      "    \"feature_map_jacobian\":false,\n",
      "    \"feature_map_jacobian_train_only\":false,\n",
      "    \"feature_map_type\":\"not_specified\",\n",
      "    \"td_prior_scale\":4.0,\n",
      "    \"feature_update\":1,\n",
      "    \"full_cov\":false,\n",
      "    \"n_samples\":5,\n",
      "    \"n_samples_eval\":0,\n",
      "    \"tau\":1,\n",
      "    \"noise_std\":1,\n",
      "    \"logging_frequency\":500,\n",
      "    \"figsize\":[\n",
      "        10,\n",
      "        4\n",
      "    ],\n",
      "    \"seed\":0,\n",
      "    \"save_path\":\"results/tmp/fashionmnist_['mnist', 'notmnist', 'kmnist']/model_fsvi_cnn_dropout__architecture_four_layers__priormean_0__priorcov_10__optimizer_adam__lr_0.0005__bs_128__indpoints_4__indtype_ood_rand__klscale_none__nsamples_5__nmarginal_1tau_1__indlim_-1.0_1.0__reg_0__seed_0__4\",\n",
      "    \"save\":true,\n",
      "    \"name\":\"\",\n",
      "    \"evaluate\":false,\n",
      "    \"resume_training\":false,\n",
      "    \"no_final_layer_bias\":false,\n",
      "    \"extra_linear_layer\":false,\n",
      "    \"map_initialization\":false,\n",
      "    \"stochastic_linearization\":true,\n",
      "    \"grad_flow_jacobian\":false,\n",
      "    \"stochastic_prior_mean\":\"not_specified\",\n",
      "    \"batch_normalization\":false,\n",
      "    \"batch_normalization_mod\":\"not_specified\",\n",
      "    \"final_layer_variational\":true,\n",
      "    \"kl_sup\":\"max\",\n",
      "    \"kl_sampled\":false,\n",
      "    \"fixed_inner_layers_variational_var\":false,\n",
      "    \"init_logvar\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"init_logvar_lin\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"init_logvar_conv\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"perturbation_param\":0.01,\n",
      "    \"debug\":true,\n",
      "    \"logroot\":null,\n",
      "    \"subdir\":null,\n",
      "    \"wandb_project\":\"not_specified\",\n",
      "    \"task\":\"fashionmnist_['mnist', 'notmnist', 'kmnist']\",\n",
      "    \"init_logvar_minval\":0.0,\n",
      "    \"init_logvar_maxval\":0.0,\n",
      "    \"init_logvar_lin_minval\":0.0,\n",
      "    \"init_logvar_lin_maxval\":0.0,\n",
      "    \"init_logvar_conv_minval\":0.0,\n",
      "    \"init_logvar_conv_maxval\":0.0,\n",
      "    \"model_type\":\"fsvi_cnn_dropout\",\n",
      "    \"n_inducing_inputs\":4,\n",
      "    \"inducing_inputs_bound\":[\n",
      "        -1.0,\n",
      "        1.0\n",
      "    ]\n",
      "} \n",
      "\n",
      "\n",
      "MAP initialization: False\n",
      "Full NTK computation: False\n",
      "Variational final layer: True\n",
      "Stochastic linearization (posterior): True\n",
      "Stochastic linearization (prior): False\n",
      "Gradient flow through Jacobian evaluation: False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "n_samples_eval: 10\n",
      "n_samples_train: 1\n",
      "n_batches_test: 10\n",
      "n_batches_ood: [10, 10, 10]\n",
      "n_eval: 1000\n",
      "ece_over_samples: False\n",
      "\n",
      "--- Training for 30 epochs ---\n",
      "\n",
      "----  --------  --------  --------  --------  --------  -------------------------------  ---------------------------  ---------------------------  ---------  -------------------------------  --------  --------  --------  ---------  -----------\n",
      "  ep    tr_nll    tr_acc    te_nll    te_acc    te ece  auc ent                          auc alea                     auc epis                       te_entr  ood_entr                             elbo      ellk        kl    time_ep    time_eval\n",
      "----  --------  --------  --------  --------  --------  -------------------------------  ---------------------------  ---------------------------  ---------  -------------------------------  --------  --------  --------  ---------  -----------\n",
      "   1     0.314    88.680     0.351    87.880     0.020  |   97.971   95.496   99.770  |  98.029   95.552   99.775  |  65.545   66.791   58.158  |      0.399  |    1.942    1.784    2.176  |     0.000     0.000     0.000     14.646        4.555\n",
      "   2     0.263    90.240     0.304    89.330     0.016  |   97.472   96.061   99.856  |  97.526   96.101   99.859  |  65.181   72.161   49.855  |      0.335  |    1.930    1.773    2.229  |     0.000     0.000     0.000     10.819        3.130\n",
      "   3     0.275    89.940     0.316    89.110     0.012  |   98.718   95.235   99.939  |  98.762   95.287   99.942  |  70.830   75.992   60.648  |      0.265  |    1.987    1.652    2.210  |     0.000     0.000     0.000     10.808        3.134\n",
      "   4     0.210    92.060     0.268    90.690     0.006  |   98.435   96.248   99.962  |  98.468   96.257   99.963  |  69.299   78.954   51.660  |      0.270  |    1.983    1.706    2.253  |     0.000     0.000     0.000     10.795        3.085\n",
      "   5     0.194    92.870     0.263    90.700     0.006  |   98.221   94.488   99.976  |  98.266   94.459   99.978  |  74.671   82.810   60.194  |      0.232  |    1.928    1.538    2.247  |     0.000     0.000     0.000     10.803        3.142\n",
      "   6     0.177    93.240     0.249    91.510     0.004  |   97.056   95.207   99.951  |  97.125   95.205   99.955  |  74.126   80.540   58.101  |      0.218  |    1.853    1.623    2.252  |     0.000     0.000     0.000     10.787        3.121\n",
      "   7     0.148    94.840     0.234    91.980     0.005  |   99.032   96.256   99.994  |  99.061   96.247   99.995  |  74.433   81.547   58.358  |      0.206  |    2.030    1.688    2.270  |     0.000     0.000     0.000     10.842        3.137\n",
      "   8     0.138    94.690     0.240    91.820     0.003  |   99.610   96.078   99.999  |  99.626   96.044   99.999  |  71.193   81.488   55.246  |      0.214  |    2.127    1.681    2.283  |     0.000     0.000     0.000     10.795        3.123\n",
      "   9     0.120    95.770     0.233    92.180     0.006  |   99.713   95.565   99.999  |  99.726   95.484   99.999  |  73.576   82.219   56.526  |      0.195  |    2.123    1.652    2.290  |     0.000     0.000     0.000     10.814        3.124\n",
      "  10     0.108    96.010     0.233    92.730     0.008  |   99.572   95.805   99.999  |  99.596   95.768   99.999  |  75.886   83.384   60.952  |      0.188  |    2.090    1.632    2.282  |     0.000     0.000     0.000     10.825        3.126\n",
      "  11     0.100    96.370     0.231    92.780     0.007  |   99.206   95.289   99.997  |  99.243   95.253   99.997  |  78.495   84.925   63.503  |      0.174  |    2.024    1.551    2.281  |     0.000     0.000     0.000     10.846        3.148\n",
      "  12     0.089    96.660     0.251    92.760     0.017  |   99.306   95.579   99.999  |  99.368   95.571   99.999  |  81.069   85.950   68.924  |      0.148  |    1.989    1.512    2.280  |     0.000     0.000     0.000     10.886        3.121\n",
      "  13     0.080    97.250     0.246    92.690     0.013  |   99.767   95.329   99.999  |  99.795   95.284   99.999  |  78.321   84.772   66.271  |      0.161  |    2.104    1.549    2.286  |     0.000     0.000     0.000     11.020        3.123\n",
      "  14     0.078    97.030     0.262    92.520     0.015  |   99.802   96.910  100.000  |  99.823   96.869  100.000  |  77.902   83.494   65.539  |      0.159  |    2.136    1.744    2.294  |     0.000     0.000     0.000     10.853        3.118\n",
      "  15     0.057    97.980     0.267    92.620     0.019  |   99.807   95.554  100.000  |  99.833   95.521  100.000  |  81.131   84.557   70.324  |      0.144  |    2.076    1.591    2.289  |     0.000     0.000     0.000     10.796        3.142\n",
      "  16     0.050    98.300     0.273    92.760     0.020  |   99.803   95.412  100.000  |  99.834   95.402  100.000  |  80.279   85.549   72.177  |      0.137  |    2.123    1.527    2.290  |     0.000     0.000     0.000     10.807        3.115\n",
      "  17     0.048    98.340     0.287    92.780     0.020  |   99.792   95.532   99.999  |  99.827   95.504   99.999  |  81.137   86.017   73.287  |      0.136  |    2.109    1.505    2.287  |     0.000     0.000     0.000     10.803        3.146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  18     0.040    98.630     0.287    92.880     0.021  |   99.824   95.997  100.000  |  99.853   95.980  100.000  |  80.965   85.824   74.767  |      0.130  |    2.145    1.589    2.294  |     0.000     0.000     0.000     10.858        3.135\n",
      "  19     0.036    98.740     0.311    92.460     0.028  |   99.781   96.260  100.000  |  99.824   96.296  100.000  |  83.279   86.812   77.118  |      0.120  |    2.101    1.544    2.291  |     0.000     0.000     0.000     10.910        3.118\n",
      "  20     0.029    99.070     0.301    93.000     0.024  |   99.741   95.347  100.000  |  99.796   95.393  100.000  |  82.693   85.986   77.913  |      0.119  |    2.099    1.516    2.292  |     0.000     0.000     0.000     10.993        3.127\n",
      "  21     0.038    98.620     0.354    92.330     0.032  |   99.770   96.439  100.000  |  99.831   96.517  100.000  |  82.407   85.959   78.724  |      0.113  |    2.137    1.595    2.294  |     0.000     0.000     0.000     11.007        3.155\n",
      "  22     0.026    99.150     0.335    93.080     0.027  |   99.930   95.851  100.000  |  99.946   95.886  100.000  |  83.921   87.746   80.955  |      0.108  |    2.185    1.484    2.293  |     0.000     0.000     0.000     11.041        3.138\n",
      "  23     0.026    99.170     0.345    92.840     0.028  |   99.930   96.664  100.000  |  99.946   96.706  100.000  |  84.434   87.937   81.950  |      0.111  |    2.192    1.593    2.293  |     0.000     0.000     0.000     10.824        3.135\n",
      "  24     0.016    99.410     0.369    92.880     0.031  |   99.915   95.219  100.000  |  99.939   95.361  100.000  |  86.019   87.812   83.652  |      0.099  |    2.168    1.415    2.291  |     0.000     0.000     0.000     10.818        3.118\n",
      "  25     0.018    99.390     0.367    93.040     0.028  |   99.849   95.998  100.000  |  99.891   96.084  100.000  |  86.434   88.280   83.823  |      0.106  |    2.124    1.507    2.287  |     0.000     0.000     0.000     10.823        3.102\n",
      "  26     0.016    99.510     0.373    93.070     0.027  |   99.936   96.258  100.000  |  99.956   96.327  100.000  |  86.305   88.652   84.503  |      0.106  |    2.196    1.517    2.290  |     0.000     0.000     0.000     10.810        3.116\n",
      "  27     0.020    99.300     0.390    92.750     0.031  |   99.890   95.711   99.999  |  99.921   95.790  100.000  |  87.724   88.895   85.825  |      0.105  |    2.167    1.495    2.288  |     0.000     0.000     0.000     10.954        3.101\n",
      "  28     0.013    99.550     0.398    93.130     0.029  |   99.951   96.260  100.000  |  99.967   96.363  100.000  |  88.536   89.370   87.080  |      0.100  |    2.179    1.558    2.286  |     0.000     0.000     0.000     11.005        3.092\n",
      "  29     0.009    99.800     0.417    93.140     0.029  |   99.938   95.357  100.000  |  99.959   95.495  100.000  |  89.759   89.093   88.097  |      0.097  |    2.146    1.464    2.283  |     0.000     0.000     0.000     10.981        3.147\n",
      "  30     0.011    99.590     0.437    93.080     0.031  |   99.652   96.257  100.000  |  99.749   96.386  100.000  |  91.011   90.940   89.392  |      0.095  |    2.054    1.480    2.278  |     0.000     0.000     0.000     10.998        3.092\n",
      "\n",
      "Changing save path from\n",
      "\n",
      "results/tmp/fashionmnist_['mnist', 'notmnist', 'kmnist']/model_fsvi_cnn_dropout__architecture_four_layers__priormean_0__priorcov_10__optimizer_adam__lr_0.0005__bs_128__indpoints_4__indtype_ood_rand__klscale_none__nsamples_5__nmarginal_1tau_1__indlim_-1.0_1.0__reg_0__seed_0__4\n",
      "\n",
      "to\n",
      "\n",
      "results/tmp/fashionmnist_['mnist', 'notmnist', 'kmnist']/model_fsvi_cnn_dropout__architecture_four_layers__priormean_0__priorcov_10__optimizer_adam__lr_0.0005__bs_128__indpoints_4__indtype_ood_rand__klscale_none__nsamples_5__nmarginal_1tau_1__indlim_-1.0_1.0__reg_0__seed_0__4__complete\n",
      "\n",
      "\n",
      "------------------- DONE -------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = os.path.abspath(os.path.dirname(os.getcwd()))\n",
    "\n",
    "if os.getcwd()[-4:] == 'fsvi':\n",
    "    pass\n",
    "else:\n",
    "    os.chdir(path)\n",
    "\n",
    "%run run_base.py\\\n",
    "--data_training fashionmnist --data_ood mnist notmnist kmnist --model fsvi_cnn_dropout --architecture four_layers --activation relu --epochs 30 --learning_rate 5e-4 --optimizer adam --batch_size 128 --prior_mean 0 --prior_cov 10 --prior_type fixed --inducing_points 4 --n_marginals 1 --kl_sup max --inducing_input_type ood_rand --inducing_input_ood_data kmnist --kl_scale none --n_samples 5 --logging_frequency 500 --seed 0 --debug --save --save_path tmp --feature_update 1 --final_layer_variational --dropout_rate 0.1 --stochastic_linearization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. (1) Randomness in non-final layers generated by dropout + (2) linearization about value sampled from variational distribution over parameters instead about variational mean + (3) gradient flow through linearization evaluation points (otherwise same as baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making GPU operations deterministic by setting os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_deterministic_reductions\"\"and os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
      "WARNING: UCI trainer could not be loaded\n",
      "WARNING: Offline RL trainer could not be loaded\n",
      "WARNING: Offline RL evaluator could not be loaded\n",
      "WARNING: Continual learning trainer could not be loaded\n",
      "\n",
      "Device: gpu\n",
      "\n",
      "Input arguments:\n",
      " {\n",
      "    \"data_training\":\"fashionmnist\",\n",
      "    \"data_ood\":[\n",
      "        \"mnist\",\n",
      "        \"notmnist\",\n",
      "        \"kmnist\"\n",
      "    ],\n",
      "    \"optimizer\":\"adam\",\n",
      "    \"optimizer_var\":\"not_specified\",\n",
      "    \"momentum\":0.0,\n",
      "    \"momentum_var\":0.0,\n",
      "    \"schedule\":\"not_specified\",\n",
      "    \"architecture\":\"four_layers\",\n",
      "    \"activation\":\"relu\",\n",
      "    \"prior_mean\":\"0\",\n",
      "    \"prior_cov\":\"10\",\n",
      "    \"prior_covs\":[\n",
      "        0.0\n",
      "    ],\n",
      "    \"prior_type\":\"fixed\",\n",
      "    \"epochs\":30,\n",
      "    \"start_var_opt\":0,\n",
      "    \"batch_size\":128,\n",
      "    \"learning_rate\":0.0005,\n",
      "    \"learning_rate_var\":0.001,\n",
      "    \"dropout_rate\":0.1,\n",
      "    \"regularization\":0,\n",
      "    \"n_marginals\":1,\n",
      "    \"n_condition\":128,\n",
      "    \"inducing_input_type\":\"ood_rand\",\n",
      "    \"inducing_input_ood_data\":[\n",
      "        \"kmnist\"\n",
      "    ],\n",
      "    \"inducing_input_ood_data_size\":50000,\n",
      "    \"kl_scale\":\"none\",\n",
      "    \"feature_map_jacobian\":false,\n",
      "    \"feature_map_jacobian_train_only\":false,\n",
      "    \"feature_map_type\":\"not_specified\",\n",
      "    \"td_prior_scale\":4.0,\n",
      "    \"feature_update\":1,\n",
      "    \"full_cov\":false,\n",
      "    \"n_samples\":5,\n",
      "    \"n_samples_eval\":0,\n",
      "    \"tau\":1,\n",
      "    \"noise_std\":1,\n",
      "    \"logging_frequency\":500,\n",
      "    \"figsize\":[\n",
      "        10,\n",
      "        4\n",
      "    ],\n",
      "    \"seed\":0,\n",
      "    \"save_path\":\"results/tmp/fashionmnist_['mnist', 'notmnist', 'kmnist']/model_fsvi_cnn_dropout__architecture_four_layers__priormean_0__priorcov_10__optimizer_adam__lr_0.0005__bs_128__indpoints_4__indtype_ood_rand__klscale_none__nsamples_5__nmarginal_1tau_1__indlim_-1.0_1.0__reg_0__seed_0__5\",\n",
      "    \"save\":true,\n",
      "    \"name\":\"\",\n",
      "    \"evaluate\":false,\n",
      "    \"resume_training\":false,\n",
      "    \"no_final_layer_bias\":false,\n",
      "    \"extra_linear_layer\":false,\n",
      "    \"map_initialization\":false,\n",
      "    \"stochastic_linearization\":true,\n",
      "    \"grad_flow_jacobian\":true,\n",
      "    \"stochastic_prior_mean\":\"not_specified\",\n",
      "    \"batch_normalization\":false,\n",
      "    \"batch_normalization_mod\":\"not_specified\",\n",
      "    \"final_layer_variational\":true,\n",
      "    \"kl_sup\":\"max\",\n",
      "    \"kl_sampled\":false,\n",
      "    \"fixed_inner_layers_variational_var\":false,\n",
      "    \"init_logvar\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"init_logvar_lin\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"init_logvar_conv\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"perturbation_param\":0.01,\n",
      "    \"debug\":true,\n",
      "    \"logroot\":null,\n",
      "    \"subdir\":null,\n",
      "    \"wandb_project\":\"not_specified\",\n",
      "    \"task\":\"fashionmnist_['mnist', 'notmnist', 'kmnist']\",\n",
      "    \"init_logvar_minval\":0.0,\n",
      "    \"init_logvar_maxval\":0.0,\n",
      "    \"init_logvar_lin_minval\":0.0,\n",
      "    \"init_logvar_lin_maxval\":0.0,\n",
      "    \"init_logvar_conv_minval\":0.0,\n",
      "    \"init_logvar_conv_maxval\":0.0,\n",
      "    \"model_type\":\"fsvi_cnn_dropout\",\n",
      "    \"n_inducing_inputs\":4,\n",
      "    \"inducing_inputs_bound\":[\n",
      "        -1.0,\n",
      "        1.0\n",
      "    ]\n",
      "} \n",
      "\n",
      "\n",
      "MAP initialization: False\n",
      "Full NTK computation: False\n",
      "Variational final layer: True\n",
      "Stochastic linearization (posterior): True\n",
      "Stochastic linearization (prior): False\n",
      "Gradient flow through Jacobian evaluation: True\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "n_samples_eval: 10\n",
      "n_samples_train: 1\n",
      "n_batches_test: 10\n",
      "n_batches_ood: [10, 10, 10]\n",
      "n_eval: 1000\n",
      "ece_over_samples: False\n",
      "\n",
      "--- Training for 30 epochs ---\n",
      "\n",
      "----  --------  --------  --------  --------  --------  -------------------------------  ---------------------------  ---------------------------  ---------  -------------------------------  --------  --------  --------  ---------  -----------\n",
      "  ep    tr_nll    tr_acc    te_nll    te_acc    te ece  auc ent                          auc alea                     auc epis                       te_entr  ood_entr                             elbo      ellk        kl    time_ep    time_eval\n",
      "----  --------  --------  --------  --------  --------  -------------------------------  ---------------------------  ---------------------------  ---------  -------------------------------  --------  --------  --------  ---------  -----------\n",
      "   1     0.362    87.220     0.422    87.190     0.035  |   98.041   93.705   99.661  |  79.009   82.709   81.874  |  99.800   95.644   99.961  |      0.462  |    1.738    1.549    1.925  |     0.000     0.000     0.000     15.483        4.458\n",
      "   2     0.306    88.900     0.341    88.470     0.021  |   98.701   95.186   99.865  |  86.565   87.584   85.711  |  99.829   96.964   99.989  |      0.383  |    1.906    1.654    2.023  |     0.000     0.000     0.000     11.086        3.124\n",
      "   3     0.294    89.300     0.341    88.650     0.006  |   99.900   96.355   99.967  |  89.572   91.460   88.478  |  99.984   96.894   99.993  |      0.303  |    2.036    1.751    2.053  |     0.000     0.000     0.000     11.105        3.090\n",
      "   4     0.229    91.360     0.283    90.330     0.005  |   99.849   96.938   99.968  |  90.275   93.129   89.506  |  99.977   97.565   99.997  |      0.275  |    2.045    1.746    2.066  |     0.000     0.000     0.000     11.080        3.111\n",
      "   5     0.220    91.830     0.279    90.430     0.005  |   99.730   96.539   99.973  |  91.488   92.863   91.027  |  99.962   97.478   99.997  |      0.254  |    2.046    1.703    2.080  |     0.000     0.000     0.000     11.109        3.124\n",
      "   6     0.204    92.180     0.267    90.860     0.005  |   99.515   96.827   99.959  |  91.149   93.529   90.945  |  99.885   97.657   99.997  |      0.235  |    1.998    1.696    2.052  |     0.000     0.000     0.000     11.084        3.115\n",
      "   7     0.174    93.530     0.249    91.500     0.006  |   99.816   97.567   99.973  |  92.290   94.440   91.528  |  99.960   98.118   99.996  |      0.220  |    2.022    1.734    2.052  |     0.000     0.000     0.000     11.077        3.114\n",
      "   8     0.162    93.860     0.249    91.630     0.004  |   99.941   97.690   99.968  |  92.874   94.626   92.154  |  99.992   98.204   99.997  |      0.229  |    2.081    1.781    2.080  |     0.000     0.000     0.000     10.983        3.124\n",
      "   9     0.144    94.800     0.245    91.950     0.003  |   99.932   97.762   99.957  |  94.386   94.916   93.540  |  99.992   98.278   99.997  |      0.217  |    2.082    1.769    2.083  |     0.000     0.000     0.000     11.017        3.144\n",
      "  10     0.137    94.930     0.246    92.350     0.006  |   99.914   97.604   99.973  |  92.982   94.466   92.186  |  99.985   98.090   99.997  |      0.210  |    2.035    1.744    2.040  |     0.000     0.000     0.000     10.907        3.114\n",
      "  11     0.112    96.060     0.237    92.570     0.007  |   99.837   96.932   99.982  |  96.061   94.948   96.164  |  99.954   97.800   99.998  |      0.185  |    2.046    1.675    2.081  |     0.000     0.000     0.000     10.902        3.128\n",
      "  12     0.109    96.160     0.244    92.610     0.011  |   99.931   96.784   99.986  |  96.349   94.808   96.065  |  99.977   97.427   99.998  |      0.169  |    2.061    1.676    2.084  |     0.000     0.000     0.000     11.090        3.104\n",
      "  13     0.100    96.500     0.247    92.820     0.012  |   99.861   96.278   99.996  |  97.300   94.654   97.453  |  99.945   97.097   99.998  |      0.164  |    2.047    1.607    2.099  |     0.000     0.000     0.000     11.120        3.145\n",
      "  14     0.095    96.490     0.260    92.350     0.010  |   99.955   97.506   99.971  |  96.553   95.443   96.515  |  99.994   98.055   99.997  |      0.176  |    2.041    1.718    2.066  |     0.000     0.000     0.000     11.112        3.125\n",
      "  15     0.078    97.140     0.258    92.650     0.015  |   99.930   97.061   99.975  |  97.627   95.586   97.580  |  99.980   97.571   99.998  |      0.156  |    2.057    1.692    2.080  |     0.000     0.000     0.000     11.126        3.143\n",
      "  16     0.063    97.840     0.266    92.900     0.017  |   99.909   97.233   99.988  |  97.848   95.618   98.010  |  99.964   97.734   99.999  |      0.142  |    2.049    1.628    2.092  |     0.000     0.000     0.000     11.119        3.140\n",
      "  17     0.065    97.600     0.282    92.620     0.019  |   99.923   97.341   99.989  |  98.283   96.171   98.236  |  99.964   97.897   99.997  |      0.144  |    2.049    1.648    2.074  |     0.000     0.000     0.000     11.119        3.144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  18     0.051    98.470     0.281    92.710     0.020  |   99.953   96.978   99.987  |  98.334   95.739   98.277  |  99.980   97.534   99.997  |      0.140  |    2.064    1.642    2.085  |     0.000     0.000     0.000     11.009        3.128\n",
      "  19     0.050    98.200     0.319    92.550     0.026  |   99.928   97.126   99.984  |  98.708   96.068   98.874  |  99.966   97.537   99.998  |      0.122  |    2.020    1.576    2.078  |     0.000     0.000     0.000     11.025        3.136\n",
      "  20     0.039    98.770     0.287    92.990     0.019  |   99.893   97.387   99.979  |  98.400   96.247   98.422  |  99.951   97.805   99.996  |      0.133  |    2.030    1.666    2.069  |     0.000     0.000     0.000     11.051        3.141\n",
      "  21     0.040    98.620     0.311    92.900     0.023  |   99.904   97.025   99.971  |  98.849   96.017   98.860  |  99.959   97.410   99.998  |      0.126  |    2.045    1.631    2.081  |     0.000     0.000     0.000     10.921        3.116\n",
      "  22     0.029    99.040     0.321    92.900     0.026  |   99.945   97.239   99.987  |  98.694   96.180   98.708  |  99.966   97.569   99.994  |      0.114  |    2.038    1.615    2.068  |     0.000     0.000     0.000     11.157        3.146\n",
      "  23     0.031    98.810     0.337    92.780     0.026  |   99.957   96.785   99.990  |  98.819   95.611   98.692  |  99.974   97.209   99.997  |      0.117  |    2.044    1.578    2.066  |     0.000     0.000     0.000     11.100        3.124\n",
      "  24     0.025    99.130     0.345    93.070     0.027  |   99.955   97.110   99.988  |  99.036   96.085   98.970  |  99.975   97.432   99.996  |      0.107  |    2.037    1.571    2.052  |     0.000     0.000     0.000     11.103        3.110\n",
      "  25     0.026    99.100     0.360    93.060     0.027  |   99.944   96.817   99.989  |  98.630   95.827   98.664  |  99.967   97.055   99.997  |      0.108  |    2.016    1.582    2.046  |     0.000     0.000     0.000     11.097        3.146\n",
      "  26     0.021    99.290     0.353    93.330     0.025  |   99.947   97.157   99.980  |  98.977   96.216   98.972  |  99.974   97.441   99.997  |      0.106  |    2.024    1.591    2.047  |     0.000     0.000     0.000     11.090        3.114\n",
      "  27     0.022    99.180     0.390    92.810     0.031  |   99.965   97.945   99.990  |  99.187   96.958   99.142  |  99.980   98.175   99.999  |      0.102  |    2.029    1.623    2.052  |     0.000     0.000     0.000     11.108        3.138\n",
      "  28     0.017    99.500     0.379    93.150     0.028  |   99.957   97.651   99.993  |  99.406   96.909   99.415  |  99.968   97.790   99.997  |      0.101  |    2.028    1.610    2.057  |     0.000     0.000     0.000     11.092        3.109\n",
      "  29     0.017    99.400     0.388    93.010     0.031  |   99.950   97.521   99.993  |  99.407   96.748   99.465  |  99.965   97.709   99.998  |      0.098  |    2.023    1.599    2.058  |     0.000     0.000     0.000     11.100        3.115\n",
      "  30     0.014    99.520     0.400    92.940     0.032  |   99.941   97.639   99.986  |  99.288   96.824   99.323  |  99.963   97.803   99.996  |      0.097  |    2.007    1.614    2.040  |     0.000     0.000     0.000     11.078        3.130\n",
      "\n",
      "Changing save path from\n",
      "\n",
      "results/tmp/fashionmnist_['mnist', 'notmnist', 'kmnist']/model_fsvi_cnn_dropout__architecture_four_layers__priormean_0__priorcov_10__optimizer_adam__lr_0.0005__bs_128__indpoints_4__indtype_ood_rand__klscale_none__nsamples_5__nmarginal_1tau_1__indlim_-1.0_1.0__reg_0__seed_0__5\n",
      "\n",
      "to\n",
      "\n",
      "results/tmp/fashionmnist_['mnist', 'notmnist', 'kmnist']/model_fsvi_cnn_dropout__architecture_four_layers__priormean_0__priorcov_10__optimizer_adam__lr_0.0005__bs_128__indpoints_4__indtype_ood_rand__klscale_none__nsamples_5__nmarginal_1tau_1__indlim_-1.0_1.0__reg_0__seed_0__5__complete\n",
      "\n",
      "\n",
      "------------------- DONE -------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = os.path.abspath(os.path.dirname(os.getcwd()))\n",
    "\n",
    "if os.getcwd()[-4:] == 'fsvi':\n",
    "    pass\n",
    "else:\n",
    "    os.chdir(path)\n",
    "\n",
    "%run run_base.py\\\n",
    "--data_training fashionmnist --data_ood mnist notmnist kmnist --model fsvi_cnn_dropout --architecture four_layers --activation relu --epochs 30 --learning_rate 5e-4 --optimizer adam --batch_size 128 --prior_mean 0 --prior_cov 10 --prior_type fixed --inducing_points 4 --n_marginals 1 --kl_sup max --inducing_input_type ood_rand --inducing_input_ood_data kmnist --kl_scale none --n_samples 5 --logging_frequency 500 --seed 0 --debug --save --save_path tmp --feature_update 1 --final_layer_variational --dropout_rate 0.1 --stochastic_linearization --grad_flow_jacobian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. (0) Context set constructed from traiing set + (1) randomness in non-final layers generated by dropout + (2) linearization about value sampled from variational distribution over parameters instead about variational mean + (3) gradient flow through linearization evaluation points (otherwise same as baseline):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making GPU operations deterministic by setting os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_deterministic_reductions\"\"and os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
      "WARNING: UCI trainer could not be loaded\n",
      "WARNING: Offline RL trainer could not be loaded\n",
      "WARNING: Offline RL evaluator could not be loaded\n",
      "WARNING: Continual learning trainer could not be loaded\n",
      "\n",
      "Device: gpu\n",
      "\n",
      "Input arguments:\n",
      " {\n",
      "    \"data_training\":\"fashionmnist\",\n",
      "    \"data_ood\":[\n",
      "        \"mnist\",\n",
      "        \"notmnist\",\n",
      "        \"kmnist\"\n",
      "    ],\n",
      "    \"optimizer\":\"adam\",\n",
      "    \"optimizer_var\":\"not_specified\",\n",
      "    \"momentum\":0.0,\n",
      "    \"momentum_var\":0.0,\n",
      "    \"schedule\":\"not_specified\",\n",
      "    \"architecture\":\"four_layers\",\n",
      "    \"activation\":\"relu\",\n",
      "    \"prior_mean\":\"0\",\n",
      "    \"prior_cov\":\"10\",\n",
      "    \"prior_covs\":[\n",
      "        0.0\n",
      "    ],\n",
      "    \"prior_type\":\"fixed\",\n",
      "    \"epochs\":30,\n",
      "    \"start_var_opt\":0,\n",
      "    \"batch_size\":128,\n",
      "    \"learning_rate\":0.0005,\n",
      "    \"learning_rate_var\":0.001,\n",
      "    \"dropout_rate\":0.1,\n",
      "    \"regularization\":0,\n",
      "    \"n_marginals\":1,\n",
      "    \"n_condition\":128,\n",
      "    \"inducing_input_type\":\"train_pixel_rand_0.5\",\n",
      "    \"inducing_input_ood_data\":[\n",
      "        \"not_specified\"\n",
      "    ],\n",
      "    \"inducing_input_ood_data_size\":50000,\n",
      "    \"kl_scale\":\"none\",\n",
      "    \"feature_map_jacobian\":false,\n",
      "    \"feature_map_jacobian_train_only\":false,\n",
      "    \"feature_map_type\":\"not_specified\",\n",
      "    \"td_prior_scale\":4.0,\n",
      "    \"feature_update\":1,\n",
      "    \"full_cov\":false,\n",
      "    \"n_samples\":5,\n",
      "    \"n_samples_eval\":0,\n",
      "    \"tau\":1,\n",
      "    \"noise_std\":1,\n",
      "    \"logging_frequency\":500,\n",
      "    \"figsize\":[\n",
      "        10,\n",
      "        4\n",
      "    ],\n",
      "    \"seed\":0,\n",
      "    \"save_path\":\"results/tmp/fashionmnist_['mnist', 'notmnist', 'kmnist']/model_fsvi_cnn_dropout__architecture_four_layers__priormean_0__priorcov_10__optimizer_adam__lr_0.0005__bs_128__indpoints_4__indtype_train_pixel_rand_0.5__klscale_none__nsamples_5__nmarginal_1tau_1__indlim_-1.0_1.0__reg_0__seed_0__1\",\n",
      "    \"save\":true,\n",
      "    \"name\":\"\",\n",
      "    \"evaluate\":false,\n",
      "    \"resume_training\":false,\n",
      "    \"no_final_layer_bias\":false,\n",
      "    \"extra_linear_layer\":false,\n",
      "    \"map_initialization\":false,\n",
      "    \"stochastic_linearization\":true,\n",
      "    \"grad_flow_jacobian\":true,\n",
      "    \"stochastic_prior_mean\":\"not_specified\",\n",
      "    \"batch_normalization\":false,\n",
      "    \"batch_normalization_mod\":\"not_specified\",\n",
      "    \"final_layer_variational\":true,\n",
      "    \"kl_sup\":\"max\",\n",
      "    \"kl_sampled\":false,\n",
      "    \"fixed_inner_layers_variational_var\":false,\n",
      "    \"init_logvar\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"init_logvar_lin\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"init_logvar_conv\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"perturbation_param\":0.01,\n",
      "    \"debug\":true,\n",
      "    \"logroot\":null,\n",
      "    \"subdir\":null,\n",
      "    \"wandb_project\":\"not_specified\",\n",
      "    \"task\":\"fashionmnist_['mnist', 'notmnist', 'kmnist']\",\n",
      "    \"init_logvar_minval\":0.0,\n",
      "    \"init_logvar_maxval\":0.0,\n",
      "    \"init_logvar_lin_minval\":0.0,\n",
      "    \"init_logvar_lin_maxval\":0.0,\n",
      "    \"init_logvar_conv_minval\":0.0,\n",
      "    \"init_logvar_conv_maxval\":0.0,\n",
      "    \"model_type\":\"fsvi_cnn_dropout\",\n",
      "    \"n_inducing_inputs\":4,\n",
      "    \"inducing_inputs_bound\":[\n",
      "        -1.0,\n",
      "        1.0\n",
      "    ]\n",
      "} \n",
      "\n",
      "\n",
      "MAP initialization: False\n",
      "Full NTK computation: False\n",
      "Variational final layer: True\n",
      "Stochastic linearization (posterior): True\n",
      "Stochastic linearization (prior): False\n",
      "Gradient flow through Jacobian evaluation: True\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "n_samples_eval: 10\n",
      "n_samples_train: 1\n",
      "n_batches_test: 10\n",
      "n_batches_ood: [10, 10, 10]\n",
      "n_eval: 1000\n",
      "ece_over_samples: False\n",
      "\n",
      "--- Training for 30 epochs ---\n",
      "\n",
      "----  --------  --------  --------  --------  --------  -------------------------------  ---------------------------  ---------------------------  ---------  -------------------------------  --------  --------  --------  ---------  -----------\n",
      "  ep    tr_nll    tr_acc    te_nll    te_acc    te ece  auc ent                          auc alea                     auc epis                       te_entr  ood_entr                             elbo      ellk        kl    time_ep    time_eval\n",
      "----  --------  --------  --------  --------  --------  -------------------------------  ---------------------------  ---------------------------  ---------  -------------------------------  --------  --------  --------  ---------  -----------\n",
      "   1     0.432    84.960     0.467    86.490     0.041  |   86.946   86.141   89.349  |  81.912   72.991   84.438  |  90.142   93.198   93.368  |      0.534  |    1.316    1.285    1.328  |     0.000     0.000     0.000     16.859        5.559\n",
      "   2     0.396    86.570     0.414    87.770     0.030  |   87.951   90.626   92.815  |  84.458   78.235   88.384  |  90.263   96.513   96.492  |      0.462  |    1.336    1.345    1.406  |     0.000     0.000     0.000     11.325        3.173\n",
      "   3     0.382    87.330     0.454    87.470     0.017  |   94.432   92.605   93.040  |  90.228   81.301   88.266  |  95.824   96.825   95.706  |      0.431  |    1.448    1.378    1.341  |     0.000     0.000     0.000     11.171        3.184\n",
      "   4     0.317    89.410     0.358    89.760     0.026  |   94.594   95.300   96.432  |  91.651   85.371   92.946  |  95.748   98.601   98.221  |      0.407  |    1.506    1.477    1.530  |     0.000     0.000     0.000     11.345        3.140\n",
      "   5     0.304    89.920     0.341    90.600     0.033  |   91.479   94.142   95.413  |  87.809   81.436   90.811  |  93.724   98.210   97.970  |      0.413  |    1.352    1.418    1.431  |     0.000     0.000     0.000     11.342        3.167\n",
      "   6     0.258    90.750     0.333    90.390     0.028  |   93.368   95.140   95.388  |  89.868   83.767   91.387  |  95.476   98.258   97.759  |      0.392  |    1.412    1.444    1.409  |     0.000     0.000     0.000     11.320        3.177\n",
      "   7     0.206    92.810     0.302    91.600     0.026  |   93.957   96.477   96.690  |  91.161   87.355   93.982  |  96.488   98.723   98.344  |      0.350  |    1.372    1.470    1.455  |     0.000     0.000     0.000     11.341        3.143\n",
      "   8     0.225    92.160     0.303    91.480     0.026  |   95.063   96.139   96.564  |  92.356   88.070   93.842  |  97.260   98.271   98.003  |      0.358  |    1.484    1.505    1.458  |     0.000     0.000     0.000     11.341        3.170\n",
      "   9     0.198    93.180     0.285    91.920     0.026  |   94.446   95.649   96.879  |  91.318   87.372   94.103  |  97.332   98.121   98.557  |      0.346  |    1.386    1.483    1.481  |     0.000     0.000     0.000     11.316        3.163\n",
      "  10     0.201    93.230     0.290    92.550     0.025  |   93.621   96.411   97.479  |  91.148   88.462   95.002  |  95.621   98.256   98.590  |      0.323  |    1.425    1.511    1.525  |     0.000     0.000     0.000     11.267        3.178\n",
      "  11     0.200    93.050     0.290    92.100     0.025  |   96.599   95.576   96.237  |  93.848   87.699   93.546  |  98.254   97.880   97.734  |      0.343  |    1.472    1.441    1.398  |     0.000     0.000     0.000     11.267        3.142\n",
      "  12     0.166    94.290     0.287    92.630     0.017  |   96.983   96.625   96.012  |  94.849   90.188   93.541  |  98.212   98.185   97.419  |      0.297  |    1.499    1.497    1.364  |     0.000     0.000     0.000     11.252        3.135\n",
      "  13     0.168    94.530     0.280    92.510     0.016  |   95.097   96.826   95.773  |  92.832   90.113   93.574  |  96.901   98.372   97.154  |      0.301  |    1.351    1.494    1.341  |     0.000     0.000     0.000     11.233        3.155\n",
      "  14     0.222    92.840     0.321    92.020     0.030  |   94.288   96.056   94.886  |  91.397   87.970   92.294  |  96.378   98.105   96.283  |      0.381  |    1.389    1.515    1.419  |     0.000     0.000     0.000     11.180        3.174\n",
      "  15     0.135    95.770     0.283    92.540     0.016  |   95.177   93.837   93.690  |  92.907   86.493   91.152  |  96.782   96.612   95.624  |      0.302  |    1.345    1.326    1.231  |     0.000     0.000     0.000     11.369        3.211\n",
      "  16     0.130    96.120     0.273    93.150     0.017  |   93.826   95.950   94.930  |  92.057   89.701   92.756  |  95.305   97.631   96.471  |      0.290  |    1.278    1.454    1.293  |     0.000     0.000     0.000     11.329        3.186\n",
      "  17     0.158    94.810     0.284    92.980     0.022  |   95.539   96.092   94.991  |  93.845   88.494   92.818  |  96.497   97.959   96.266  |      0.309  |    1.389    1.438    1.348  |     0.000     0.000     0.000     11.309        3.156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  18     0.115    96.380     0.272    93.190     0.012  |   96.051   96.352   95.789  |  94.258   90.044   93.582  |  97.225   98.022   97.157  |      0.256  |    1.344    1.440    1.330  |     0.000     0.000     0.000     11.192        3.184\n",
      "  19     0.113    96.630     0.278    93.170     0.014  |   95.722   95.783   94.468  |  94.042   89.741   92.216  |  96.854   97.589   96.054  |      0.280  |    1.325    1.399    1.243  |     0.000     0.000     0.000     11.194        3.182\n",
      "  20     0.151    94.970     0.287    93.060     0.022  |   95.896   96.720   96.320  |  94.299   90.203   94.094  |  96.814   98.312   97.517  |      0.307  |    1.431    1.496    1.415  |     0.000     0.000     0.000     11.266        3.184\n",
      "  21     0.097    97.300     0.289    92.840     0.011  |   96.576   97.772   96.272  |  95.185   93.321   94.343  |  97.216   98.635   97.324  |      0.268  |    1.405    1.547    1.322  |     0.000     0.000     0.000     11.370        3.180\n",
      "  22     0.105    96.810     0.293    92.890     0.013  |   96.997   96.612   94.945  |  95.575   91.155   92.992  |  97.656   97.970   96.115  |      0.270  |    1.396    1.471    1.263  |     0.000     0.000     0.000     11.369        3.152\n",
      "  23     0.118    96.300     0.306    92.930     0.009  |   95.641   96.913   95.543  |  94.315   92.468   93.650  |  96.141   97.928   96.588  |      0.255  |    1.353    1.511    1.293  |     0.000     0.000     0.000     11.352        3.206\n",
      "  24     0.091    97.250     0.305    92.990     0.005  |   97.419   96.815   95.248  |  95.891   91.886   93.280  |  97.962   98.126   96.289  |      0.251  |    1.387    1.419    1.257  |     0.000     0.000     0.000     11.277        3.144\n",
      "  25     0.093    97.420     0.300    92.710     0.008  |   95.969   95.242   93.941  |  94.693   89.727   91.884  |  96.500   97.025   95.346  |      0.266  |    1.338    1.377    1.210  |     0.000     0.000     0.000     11.254        3.164\n",
      "  26     0.080    97.940     0.297    93.100     0.016  |   95.937   95.714   94.043  |  94.814   90.839   92.055  |  96.143   97.014   95.159  |      0.283  |    1.356    1.434    1.224  |     0.000     0.000     0.000     11.242        3.177\n",
      "  27     0.082    97.660     0.308    92.710     0.009  |   96.459   96.125   95.176  |  95.311   91.684   93.322  |  96.724   97.345   96.173  |      0.270  |    1.423    1.486    1.296  |     0.000     0.000     0.000     11.255        3.144\n",
      "  28     0.081    97.700     0.309    93.150     0.007  |   94.900   93.700   93.862  |  93.678   89.717   92.038  |  95.367   95.278   94.988  |      0.233  |    1.285    1.291    1.217  |     0.000     0.000     0.000     11.216        3.189\n",
      "  29     0.097    96.840     0.325    93.020     0.008  |   96.644   96.476   95.710  |  95.775   92.760   94.164  |  96.648   97.233   96.311  |      0.266  |    1.451    1.493    1.333  |     0.000     0.000     0.000     11.223        3.179\n",
      "  30     0.079    97.890     0.313    93.030     0.005  |   95.576   95.778   94.706  |  94.461   91.867   92.997  |  95.868   97.016   95.590  |      0.255  |    1.302    1.401    1.238  |     0.000     0.000     0.000     11.186        3.145\n",
      "\n",
      "Changing save path from\n",
      "\n",
      "results/tmp/fashionmnist_['mnist', 'notmnist', 'kmnist']/model_fsvi_cnn_dropout__architecture_four_layers__priormean_0__priorcov_10__optimizer_adam__lr_0.0005__bs_128__indpoints_4__indtype_train_pixel_rand_0.5__klscale_none__nsamples_5__nmarginal_1tau_1__indlim_-1.0_1.0__reg_0__seed_0__1\n",
      "\n",
      "to\n",
      "\n",
      "results/tmp/fashionmnist_['mnist', 'notmnist', 'kmnist']/model_fsvi_cnn_dropout__architecture_four_layers__priormean_0__priorcov_10__optimizer_adam__lr_0.0005__bs_128__indpoints_4__indtype_train_pixel_rand_0.5__klscale_none__nsamples_5__nmarginal_1tau_1__indlim_-1.0_1.0__reg_0__seed_0__1__complete\n",
      "\n",
      "\n",
      "------------------- DONE -------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = os.path.abspath(os.path.dirname(os.getcwd()))\n",
    "\n",
    "if os.getcwd()[-4:] == 'fsvi':\n",
    "    pass\n",
    "else:\n",
    "    os.chdir(path)\n",
    "\n",
    "%run run_base.py\\\n",
    "--data_training fashionmnist --data_ood mnist notmnist kmnist --model fsvi_cnn_dropout --architecture four_layers --activation relu --epochs 30 --learning_rate 5e-4 --optimizer adam --batch_size 128 --prior_mean 0 --prior_cov 10 --prior_type fixed --inducing_points 4 --n_marginals 1 --kl_sup max --inducing_input_type train_pixel_rand_0.5 --kl_scale none --n_samples 5 --logging_frequency 500 --seed 0 --debug --save --save_path tmp --feature_update 1 --final_layer_variational --dropout_rate 0.1 --stochastic_linearization --grad_flow_jacobian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training with more MC samples for expected log-likelihood estimation (otherwise same as baseline):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making GPU operations deterministic by setting os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_deterministic_reductions\"\"and os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
      "WARNING: UCI trainer could not be loaded\n",
      "WARNING: Offline RL trainer could not be loaded\n",
      "WARNING: Offline RL evaluator could not be loaded\n",
      "WARNING: Continual learning trainer could not be loaded\n",
      "\n",
      "Device: gpu\n",
      "\n",
      "Input arguments:\n",
      " {\n",
      "    \"data_training\":\"fashionmnist\",\n",
      "    \"data_ood\":[\n",
      "        \"mnist\",\n",
      "        \"notmnist\",\n",
      "        \"kmnist\"\n",
      "    ],\n",
      "    \"optimizer\":\"adam\",\n",
      "    \"optimizer_var\":\"not_specified\",\n",
      "    \"momentum\":0.0,\n",
      "    \"momentum_var\":0.0,\n",
      "    \"schedule\":\"not_specified\",\n",
      "    \"architecture\":\"four_layers\",\n",
      "    \"activation\":\"relu\",\n",
      "    \"prior_mean\":\"0\",\n",
      "    \"prior_cov\":\"10\",\n",
      "    \"prior_covs\":[\n",
      "        0.0\n",
      "    ],\n",
      "    \"prior_type\":\"fixed\",\n",
      "    \"epochs\":30,\n",
      "    \"start_var_opt\":0,\n",
      "    \"batch_size\":128,\n",
      "    \"learning_rate\":0.0005,\n",
      "    \"learning_rate_var\":0.001,\n",
      "    \"dropout_rate\":0.0,\n",
      "    \"regularization\":0,\n",
      "    \"n_marginals\":1,\n",
      "    \"n_condition\":128,\n",
      "    \"inducing_input_type\":\"ood_rand\",\n",
      "    \"inducing_input_ood_data\":[\n",
      "        \"kmnist\"\n",
      "    ],\n",
      "    \"inducing_input_ood_data_size\":50000,\n",
      "    \"kl_scale\":\"none\",\n",
      "    \"feature_map_jacobian\":false,\n",
      "    \"feature_map_jacobian_train_only\":false,\n",
      "    \"feature_map_type\":\"not_specified\",\n",
      "    \"td_prior_scale\":4.0,\n",
      "    \"feature_update\":1,\n",
      "    \"full_cov\":false,\n",
      "    \"n_samples\":10,\n",
      "    \"n_samples_eval\":0,\n",
      "    \"tau\":1,\n",
      "    \"noise_std\":1,\n",
      "    \"logging_frequency\":500,\n",
      "    \"figsize\":[\n",
      "        10,\n",
      "        4\n",
      "    ],\n",
      "    \"seed\":0,\n",
      "    \"save_path\":\"results/tmp/fashionmnist_['mnist', 'notmnist', 'kmnist']/model_fsvi_cnn__architecture_four_layers__priormean_0__priorcov_10__optimizer_adam__lr_0.0005__bs_128__indpoints_4__indtype_ood_rand__klscale_none__nsamples_10__nmarginal_1tau_1__indlim_-1.0_1.0__reg_0__seed_0__4\",\n",
      "    \"save\":true,\n",
      "    \"name\":\"\",\n",
      "    \"evaluate\":false,\n",
      "    \"resume_training\":false,\n",
      "    \"final_layer_bias\":false,\n",
      "    \"extra_linear_layer\":false,\n",
      "    \"map_initialization\":false,\n",
      "    \"stochastic_linearization\":false,\n",
      "    \"grad_flow_jacobian\":false,\n",
      "    \"stochastic_prior_mean\":\"not_specified\",\n",
      "    \"batch_normalization\":false,\n",
      "    \"batch_normalization_mod\":\"not_specified\",\n",
      "    \"final_layer_variational\":false,\n",
      "    \"kl_sup\":\"max\",\n",
      "    \"kl_sampled\":false,\n",
      "    \"fixed_inner_layers_variational_var\":false,\n",
      "    \"init_logvar\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"init_logvar_lin\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"init_logvar_conv\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"perturbation_param\":0.01,\n",
      "    \"debug\":true,\n",
      "    \"logroot\":null,\n",
      "    \"subdir\":null,\n",
      "    \"wandb_project\":\"not_specified\",\n",
      "    \"task\":\"fashionmnist_['mnist', 'notmnist', 'kmnist']\",\n",
      "    \"init_logvar_minval\":0.0,\n",
      "    \"init_logvar_maxval\":0.0,\n",
      "    \"init_logvar_lin_minval\":0.0,\n",
      "    \"init_logvar_lin_maxval\":0.0,\n",
      "    \"init_logvar_conv_minval\":0.0,\n",
      "    \"init_logvar_conv_maxval\":0.0,\n",
      "    \"model_type\":\"fsvi_cnn\",\n",
      "    \"n_inducing_inputs\":4,\n",
      "    \"inducing_inputs_bound\":[\n",
      "        -1.0,\n",
      "        1.0\n",
      "    ]\n",
      "} \n",
      "\n",
      "\n",
      "MAP initialization: False\n",
      "Full NTK computation: False\n",
      "Variational final layer: False\n",
      "Stochastic linearization (posterior): False\n",
      "Stochastic linearization (prior): False\n",
      "Gradient flow through Jacobian evaluation: False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "n_samples_eval: 10\n",
      "n_samples_train: 1\n",
      "n_batches_test: 10\n",
      "n_batches_ood: [10, 10, 10]\n",
      "n_eval: 1000\n",
      "ece_over_samples: False\n",
      "\n",
      "--- Training for 30 epochs ---\n",
      "\n",
      "----  --------  --------  --------  --------  --------  -------------------------------  ---------------------------  ---------------------------  ---------  -------------------------------  --------  --------  --------  ---------  -----------\n",
      "  ep    tr_nll    tr_acc    te_nll    te_acc    te ece  auc ent                          auc alea                     auc epis                       te_entr  ood_entr                             elbo      ellk        kl    time_ep    time_eval\n",
      "----  --------  --------  --------  --------  --------  -------------------------------  ---------------------------  ---------------------------  ---------  -------------------------------  --------  --------  --------  ---------  -----------\n",
      "   1     0.317    88.750     0.348    87.870     0.018  |   95.865   94.239   99.551  |  95.949   94.309   99.558  |  77.737   81.894   83.607  |      0.390  |    1.686    1.566    2.022  |     0.000     0.000     0.000     34.573        4.479\n",
      "   2     0.277    89.570     0.304    89.430     0.009  |   98.037   95.870   99.814  |  98.082   95.911   99.818  |  70.374   79.882   70.003  |      0.318  |    1.910    1.655    2.184  |     0.000     0.000     0.000     29.008        1.946\n",
      "   3     0.286    89.560     0.316    89.110     0.021  |   98.930   94.771   99.884  |  98.968   94.834   99.889  |  73.605   80.515   72.461  |      0.237  |    1.984    1.529    2.164  |     0.000     0.000     0.000     29.021        1.962\n",
      "   4     0.198    92.320     0.261    90.980     0.006  |   99.185   95.304   99.958  |  99.213   95.344   99.960  |  71.842   81.717   65.675  |      0.231  |    2.046    1.550    2.231  |     0.000     0.000     0.000     29.036        1.958\n",
      "   5     0.188    93.080     0.266    90.510     0.013  |   99.566   94.246   99.980  |  99.581   94.274   99.981  |  72.742   82.692   65.786  |      0.219  |    2.117    1.494    2.250  |     0.000     0.000     0.000     29.032        1.944\n",
      "   6     0.179    92.980     0.255    91.200     0.013  |   98.744   95.101   99.950  |  98.780   95.149   99.952  |  75.021   82.612   68.574  |      0.197  |    2.002    1.541    2.235  |     0.000     0.000     0.000     29.026        1.927\n",
      "   7     0.146    94.740     0.238    91.710     0.013  |   99.692   95.864   99.992  |  99.703   95.880   99.993  |  75.847   84.374   67.057  |      0.186  |    2.136    1.609    2.263  |     0.000     0.000     0.000     29.010        1.919\n",
      "   8     0.141    94.430     0.244    91.310     0.019  |   99.796   94.093   99.994  |  99.806   94.112   99.994  |  73.726   84.269   64.525  |      0.179  |    2.166    1.460    2.272  |     0.000     0.000     0.000     29.012        1.937\n",
      "   9     0.101    96.390     0.242    91.950     0.019  |   99.774   95.146   99.999  |  99.782   95.143   99.999  |  77.423   86.857   68.286  |      0.162  |    2.164    1.467    2.276  |     0.000     0.000     0.000     29.022        1.971\n",
      "  10     0.101    96.270     0.254    91.980     0.022  |   99.702   93.991   99.994  |  99.714   94.003   99.995  |  78.860   86.987   70.947  |      0.157  |    2.135    1.328    2.260  |     0.000     0.000     0.000     29.039        1.940\n",
      "  11     0.077    97.200     0.247    92.330     0.023  |   99.782   94.778   99.998  |  99.793   94.808   99.998  |  79.948   86.686   67.354  |      0.139  |    2.138    1.400    2.275  |     0.000     0.000     0.000     29.072        1.948\n",
      "  12     0.069    97.450     0.267    92.300     0.030  |   99.683   95.123   99.999  |  99.700   95.161   99.999  |  81.176   87.572   69.255  |      0.122  |    2.122    1.373    2.278  |     0.000     0.000     0.000     29.235        1.968\n",
      "  13     0.055    98.170     0.274    92.400     0.029  |   99.838   95.105   99.999  |  99.850   95.146   99.999  |  80.809   87.602   68.576  |      0.123  |    2.139    1.389    2.277  |     0.000     0.000     0.000     29.221        1.951\n",
      "  14     0.056    97.970     0.292    91.880     0.035  |   99.891   96.190   99.999  |  99.901   96.218  100.000  |  79.200   87.884   62.688  |      0.120  |    2.176    1.458    2.285  |     0.000     0.000     0.000     29.216        1.933\n",
      "  15     0.035    98.870     0.302    92.250     0.036  |   99.588   94.649   99.998  |  99.616   94.713   99.998  |  83.068   87.705   66.146  |      0.106  |    2.083    1.346    2.279  |     0.000     0.000     0.000     29.124        1.941\n",
      "  16     0.033    98.890     0.329    92.390     0.039  |   99.692   95.324   99.998  |  99.719   95.422   99.999  |  83.061   88.314   65.673  |      0.096  |    2.097    1.313    2.281  |     0.000     0.000     0.000     29.120        1.947\n",
      "  17     0.030    98.920     0.345    92.120     0.040  |   99.905   95.866   99.999  |  99.916   95.966  100.000  |  81.622   88.400   65.436  |      0.095  |    2.165    1.354    2.283  |     0.000     0.000     0.000     29.069        1.960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  18     0.021    99.250     0.374    92.160     0.044  |   99.796   95.795  100.000  |  99.823   95.916  100.000  |  83.415   88.548   61.576  |      0.086  |    2.115    1.345    2.287  |     0.000     0.000     0.000     29.227        1.962\n",
      "  19     0.031    98.830     0.381    91.990     0.045  |   99.739   94.693   99.998  |  99.772   94.826   99.999  |  84.177   87.820   64.174  |      0.087  |    2.100    1.263    2.280  |     0.000     0.000     0.000     29.048        1.984\n",
      "  20     0.020    99.350     0.389    92.400     0.042  |   99.820   95.453   99.999  |  99.848   95.619  100.000  |  82.050   87.586   59.112  |      0.084  |    2.127    1.314    2.287  |     0.000     0.000     0.000     29.046        1.941\n",
      "  21     0.027    99.000     0.431    91.740     0.050  |   99.829   96.686  100.000  |  99.858   96.774  100.000  |  80.567   87.653   50.664  |      0.081  |    2.140    1.478    2.294  |     0.000     0.000     0.000     28.995        1.956\n",
      "  22     0.011    99.700     0.451    92.170     0.049  |   99.826   96.399  100.000  |  99.858   96.517  100.000  |  83.158   88.445   51.154  |      0.072  |    2.123    1.411    2.294  |     0.000     0.000     0.000     29.122        1.965\n",
      "  23     0.015    99.490     0.479    92.110     0.050  |   99.887   96.044  100.000  |  99.908   96.223  100.000  |  83.746   88.617   52.663  |      0.070  |    2.140    1.344    2.292  |     0.000     0.000     0.000     29.216        1.969\n",
      "  24     0.020    99.360     0.478    92.000     0.050  |   99.818   95.817  100.000  |  99.851   96.001  100.000  |  83.961   88.743   56.827  |      0.071  |    2.107    1.301    2.289  |     0.000     0.000     0.000     29.209        1.957\n",
      "  25     0.008    99.750     0.492    92.350     0.047  |   99.826   95.731  100.000  |  99.861   95.899  100.000  |  79.438   87.473   49.882  |      0.072  |    2.115    1.335    2.293  |     0.000     0.000     0.000     30.125        1.956\n",
      "  26     0.005    99.870     0.481    92.530     0.047  |   99.859   95.719  100.000  |  99.888   95.862  100.000  |  80.769   87.385   48.260  |      0.068  |    2.143    1.347    2.294  |     0.000     0.000     0.000     30.154        1.944\n",
      "  27     0.020    99.270     0.500    91.840     0.053  |   99.674   95.710  100.000  |  99.725   95.898  100.000  |  83.073   88.398   50.635  |      0.070  |    2.060    1.305    2.292  |     0.000     0.000     0.000     30.168        1.952\n",
      "  28     0.010    99.710     0.536    92.140     0.052  |   99.838   96.684  100.000  |  99.874   96.850  100.000  |  83.005   89.431   48.403  |      0.065  |    2.096    1.372    2.293  |     0.000     0.000     0.000     30.045        1.952\n",
      "  29     0.005    99.860     0.523    92.370     0.050  |   99.825   95.957  100.000  |  99.859   96.124  100.000  |  80.392   88.326   42.597  |      0.064  |    2.118    1.345    2.296  |     0.000     0.000     0.000     29.958        1.961\n",
      "  30     0.006    99.780     0.567    92.310     0.052  |   99.845   96.463  100.000  |  99.880   96.663  100.000  |  80.857   89.049   42.545  |      0.060  |    2.120    1.335    2.296  |     0.000     0.000     0.000     29.929        1.934\n",
      "\n",
      "Changing save path from\n",
      "\n",
      "results/tmp/fashionmnist_['mnist', 'notmnist', 'kmnist']/model_fsvi_cnn__architecture_four_layers__priormean_0__priorcov_10__optimizer_adam__lr_0.0005__bs_128__indpoints_4__indtype_ood_rand__klscale_none__nsamples_10__nmarginal_1tau_1__indlim_-1.0_1.0__reg_0__seed_0__4\n",
      "\n",
      "to\n",
      "\n",
      "results/tmp/fashionmnist_['mnist', 'notmnist', 'kmnist']/model_fsvi_cnn__architecture_four_layers__priormean_0__priorcov_10__optimizer_adam__lr_0.0005__bs_128__indpoints_4__indtype_ood_rand__klscale_none__nsamples_10__nmarginal_1tau_1__indlim_-1.0_1.0__reg_0__seed_0__4__complete\n",
      "\n",
      "\n",
      "------------------- DONE -------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = os.path.abspath(os.path.dirname(os.getcwd()))\n",
    "\n",
    "if os.getcwd()[-4:] == 'fsvi':\n",
    "    pass\n",
    "else:\n",
    "    os.chdir(path)\n",
    "\n",
    "%run run_base.py\\\n",
    "--data_training fashionmnist --data_ood mnist notmnist kmnist --model fsvi_cnn --architecture four_layers --activation relu --epochs 30 --learning_rate 5e-4 --optimizer adam --batch_size 128 --prior_mean 0 --prior_cov 10 --prior_type fixed --inducing_points 4 --n_marginals 1 --kl_sup max --inducing_input_type ood_rand --inducing_input_ood_data kmnist --kl_scale none --n_samples 10 --logging_frequency 500 --seed 0 --debug --save --save_path tmp --feature_update 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you for reviewing our results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fsvi]",
   "language": "python",
   "name": "conda-env-fsvi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
